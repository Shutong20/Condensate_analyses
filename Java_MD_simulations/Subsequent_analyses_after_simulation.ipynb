{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "import os\n",
    "import sys\n",
    "import igraph\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from scipy.cluster import hierarchy\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking for link files..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters that will be overwritten\n",
    "Filename = 'box_Viscosity_trial22_8-1_Long'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Consider following for batch processing in the future\n",
    "# data_folder = './Ensemble-node-links'\n",
    "# for fname in os.listdir(data_folder):\n",
    "#     path = os.path.join(data_folder,fname)\n",
    "#     if os.path.isdir(path):\n",
    "#         path_true = path;\n",
    "        \n",
    "# Single file processing - directly specifying path for a folder that contains 'Links' and 'Coords' files\n",
    "# path_true = '/Volumes/G-DRIVE USB/Experimental Data - Holt Lab/Simulation Results/Box simulation results/box Visocsity trial22 (newjar5)/synDrop_whole_system_simulation/' + Filename + '/'+ Filename + '-NL'\n",
    "# path_true = '/Users/tongshu/Documents/Lab project 2020/Levy Droplets/Simulations/Example data for analysis debugging/' + Filename + '/'+ Filename + '-NL'\n",
    "path_true = '/Users/shut01/Documents/Levy simulation folder/Re-analyze simulation data/' + Filename + '/' + Filename + '-NL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for f in os.listdir(path_true):\n",
    "#     if 'Links' in f:\n",
    "    if 'Links' in f and f.split('.')[1][2]=='0':  # Only extract the files with time *.**000s to avoid mistakes after too many files processed\n",
    "        files.append(os.path.join(path_true,f))\n",
    "        \n",
    "# Sort files\n",
    "files = sorted(files)\n",
    "print(f'Number of files found: {len(files)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dataframe with time and file paths for links and coordinates..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'links_file_path': files,\n",
    "    'coord_file_path': [f.replace('Links','Coords') for f in files],\n",
    "    'time': [float(f.split('Links')[1]) for f in files]\n",
    "})\n",
    "print(df.shape)\n",
    "df.head()\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "#     print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying initial parameters for the simulation, check everytime in different conditions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 0.86 #the length of each side of cubic simulation box in the unit of um (0.86um for larger system )\n",
    "N_A = 390 # \"N_A\" is the total number of nodes that are A (hexamer) (located at the first part of nodes, from 0 to N_A-1)\n",
    "N_B = int(N_A*3) #\"N_B\" is the total number of nodes that are B (dimer) (located at the second part of nodes, from N_A to N_A+N_B-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading link files and converting into graphs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    # Load and format the data\n",
    "    df_links = pd.read_csv(row['links_file_path'],\n",
    "                           header = None,\n",
    "                           names = ['source','target'])\n",
    "    df_coord = pd.read_csv(row['coord_file_path'],\n",
    "                           header = None,\n",
    "                           # There is an extra comma that can be removed at\n",
    "                           # data generation time\n",
    "                           names = ['node','x','y','z','diffusivity','dummy'],\n",
    "                           index_col = 0)\n",
    "    df_coord = df_coord.drop(columns=['dummy'])\n",
    "    \n",
    "    # Convert to a graph\n",
    "    nnodes = df_coord.shape[0]\n",
    "    g = igraph.Graph(nnodes, directed=False)\n",
    "    edges = [(df_links.source[idx],df_links.target[idx]) for idx in df_links.index]\n",
    "    length = [\n",
    "        np.sqrt(\n",
    "            np.power(df_coord.loc[edge[0],['x','y','z']].values-df_coord.loc[edge[1],['x','y','z']].values,2).sum()\n",
    "        )\n",
    "        for edge in edges\n",
    "    ]\n",
    "    g.add_edges(edges)\n",
    "    g.es['length'] = length\n",
    "    g.vs['diffusivity'] = df_coord.diffusivity.values\n",
    "    g.vs['coordinate'] = list(zip(df_coord.x.values, df_coord.y.values, df_coord.z.values))\n",
    "    df.loc[index,'graph'] = g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After incoorporating link and coord information into the graph colume, nodes without appearing in the link files needs to be separated since they are ribosomes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if nnodes > N_A+N_B: #If number of total nodes is larger than summation of total dimer and hexamer nodes\n",
    "    numUseful_nodes = N_A+N_B  #Identify the nodes that locate at the front part of nodes file, including dimer and hexamer\n",
    "    for g_temp in df.graph:\n",
    "        g_temp.delete_vertices(list(range(numUseful_nodes,len(g_temp.vs)))) #delete the nodes in the graph which have not participated in the edges: nodes number from 'numUseful_nodes' to 'total_nodes_number'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the molecular concentration within random fixed volume changing with time ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_box = L/4\n",
    "r_center = [[L/4,L/4,L/4],[-L/4,L/4,L/4],[L/4,-L/4,L/4],[-L/4,-L/4,L/4],[L/4,L/4,-L/4],[-L/4,L/4,-L/4],[L/4,-L/4,-L/4],[-L/4,-L/4,-L/4]] #center of the investigate areas\n",
    "N_center = [] #N_center is a list of list: [[],[],[],...] each list element contains the number of nodes within a sphere with center being r_center\n",
    "for i in range(len(r_center)): \n",
    "    N_center.append(list())\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "# for index, row in tqdm(df.head(n=5).iterrows(), total=5):\n",
    "    g = row['graph']\n",
    "    Coord = g.vs['coordinate']\n",
    "    for i in range(len(r_center)):\n",
    "        r_center_temp = r_center[i]\n",
    "##This is to count nodes within each sphere with L/4 radius         \n",
    "#         length_vector = [[j1-r_center_temp[0],j2-r_center_temp[1],j3-r_center_temp[2]] for [j1,j2,j3] in Coord]\n",
    "#         length = [np.sqrt(pow(k1,2)+pow(k2,2)+pow(k3,2)) for [k1,k2,k3] in length_vector]\n",
    "#         N_center[i].append(len([elem for elem in length if elem < r_box]))\n",
    "##This is to count nodes within each cubic with L/2 length\n",
    "        length_vector = [[j1-r_center_temp[0],j2-r_center_temp[1],j3-r_center_temp[2]] for [j1,j2,j3] in Coord]\n",
    "        length_max = [max([np.abs(k1),np.abs(k2),np.abs(k3)]) for [k1,k2,k3] in length_vector]\n",
    "        N_center[i].append(len([elem for elem in length_max if elem < r_box]))\n",
    "        \n",
    "# check = [sum(x) for x in zip(*N_center)]\n",
    "# print(check)\n",
    "        \n",
    "fig, ax = plt.subplots(1,1, figsize=(6,4))\n",
    "for i in range(len(r_center)):\n",
    "#     concen_temp = [k*3/(4*pow(r_box,3)*602*3.14) for k in N_center[i]] #Calculate concentration within the sphere with uM unit\n",
    "    concen_temp = [k/(pow(r_box*2,3)*602) for k in N_center[i]] #Calculate concentration within the cubic with uM unit\n",
    "    ax.plot(df.time,concen_temp,label=f'center at {r_center[i]}')\n",
    "    ax.set_ylabel(f'Molecular concentration within cubic \\n with length L/4={r_box} (ÂµM)', fontsize=15)\n",
    "    ax.set_xlabel('Time (s)', fontsize=15)\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.9))\n",
    "    ax.set_title(f'Dynamic of local molecular concentration')\n",
    "ave_con = len(Coord)/(pow(L,3)*602)\n",
    "ax.plot(df.time,ave_con*np.ones(len(df.time))) #This is the average concentration of all molecules within the system\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate number of neighbors based on elements directly from graph or within largest cluster in each time frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here the first part of nodes (A) are hexamers and remainings (B) are dimers\n",
    "N_neighbor_time = [] #At each time points, number of neighbors of all nodes\n",
    "giant_node = [] #At each time points, node index within the largest cluster\n",
    "N_CONNECT_TOTAL = [] #Record number of nodes in the shortest path connecting two nodes in all time points\n",
    "ORDER = [] #Record nodes order in the rearranged cluster graphs\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "# for index, row in tqdm(df.head(n=5).iterrows(), total=5):\n",
    "    g = row['graph']\n",
    "    N_neighbor = [len(g.neighborhood(i))-1 for i in range(0,g.vcount())] #The first value in the g.neighborhood is the node index itself\n",
    "    N_neighbor_time.append(N_neighbor)\n",
    "    \n",
    "    Direct_neighbor = np.zeros((g.vcount(),g.vcount())) #Direct_neighbor[i,j] is 1 if (i,j) are direct neighbors\n",
    "    for i in range(0,g.vcount()):\n",
    "        temp_neighbor = g.neighborhood(i)[1:]\n",
    "        for j in temp_neighbor:\n",
    "            Direct_neighbor[i][j] = 1 \n",
    "    \n",
    "    ccs = g.clusters()\n",
    "    ccslistsize = list(ccs.sizes())\n",
    "    giant_node.append(ccs[ccslistsize.index(max(ccslistsize))])\n",
    "    \n",
    "    # Calculate shortest paths length between nodes\n",
    "    spl_total1 = []\n",
    "    N_connect_total = []\n",
    "    for v_source in range(0,g.vcount()):\n",
    "        spl = g.shortest_paths_dijkstra(v_source, g.vs(), weights=g.es['length'])\n",
    "        spl_total1.append(spl)\n",
    "        N_connect = [float(len(i)) for i in g.get_shortest_paths(v_source, g.vs())] #Number of nodes connecting v_source and v_target (including v_source and v_target)\n",
    "        N_connect_total.append(N_connect)\n",
    "    distances = np.array(spl_total1).reshape((g.vcount(),g.vcount()))\n",
    "    N_connect_total = np.array(N_connect_total).reshape((g.vcount(),g.vcount()))\n",
    "\n",
    "    # Replace infinities with a very large distance\n",
    "    distances[np.isinf(distances)] = distances[~np.isinf(distances)].max()\n",
    "    \n",
    "    # Clustering\n",
    "    threshold = 1\n",
    "    fig, axs = plt.subplots(1,5,figsize=(35,6))\n",
    "    sys.setrecursionlimit(10000) #Required for much larger system as trail20_(5-5) which has 1170 type A molecules\n",
    "    linkage = hierarchy.linkage(distances, method=\"single\")\n",
    "    clusters = hierarchy.fcluster(linkage, threshold, criterion=\"distance\")\n",
    "    dend = hierarchy.dendrogram(linkage, color_threshold=threshold, ax=axs[4])\n",
    "    order = dend['leaves']\n",
    "    ORDER = np.append(ORDER,order,axis=0)\n",
    "    distances = distances[order,:]\n",
    "    distances = distances[:,order]\n",
    "    N_connect_total = N_connect_total[order,:]\n",
    "    N_connect_total = N_connect_total[:,order]\n",
    "    Direct_neighbor = Direct_neighbor[order,:]\n",
    "    Direct_neighbor = Direct_neighbor[:,order]\n",
    "    if len(N_CONNECT_TOTAL)==0:\n",
    "        N_CONNECT_TOTAL = N_connect_total\n",
    "    else:\n",
    "        N_CONNECT_TOTAL = np.append(N_CONNECT_TOTAL,N_connect_total,axis=0)\n",
    "\n",
    "    \n",
    "    #Calculate the direct neighbor numbers and the label for each nodes\n",
    "    N_direct_neighbor = [N_neighbor[i] for i in order]\n",
    "    N_direct_neighbor_matrix = np.tile(np.array([N_direct_neighbor]).transpose(),(1,np.int(g.vcount())))\n",
    "    N_direct_neighbor_per = []\n",
    "    N_label = []\n",
    "    for i in range(0,len(order)):\n",
    "        if order[i]<N_A:\n",
    "            N_direct_neighbor_per.append(N_direct_neighbor[i]/6)\n",
    "            N_label.append(1) #'A' is labled as 1\n",
    "        else:\n",
    "            N_direct_neighbor_per.append(N_direct_neighbor[i]/2)\n",
    "            N_label.append(0) #'B' is labled as 0\n",
    "    N_direct_neighbor_per_matrix = np.tile(np.array([N_direct_neighbor_per]).transpose(),(1,np.int(g.vcount())))\n",
    "    N_label_matrix = np.tile(np.array([N_label]).transpose(),(1,np.int(g.vcount())))\n",
    "\n",
    "#   Plot all the analysis results  \n",
    "    #After clustering, convert the largest distances (& not indirectly connected nodes) to nan and plot as white in the colormap\n",
    "    max_index = np.where(distances == np.amax(distances)) \n",
    "    distances[max_index] = np.nan\n",
    "    no_connect_index = np.where(N_connect_total == 0)\n",
    "    N_connect_total[no_connect_index] = np.nan\n",
    "    current_cmap = plt.cm.get_cmap()\n",
    "    current_cmap.set_bad(color='white')\n",
    "    \n",
    "    [x,y] = np.where(Direct_neighbor == 1) #Extract x,y node position for those are direct neighbors\n",
    "    \n",
    "    sc0 = axs[0].imshow(distances)\n",
    "#     sc1 = axs[1].imshow(N_label_matrix)\n",
    "    sc1 = axs[1].scatter(x, y, s=0.1) \n",
    "    sc2 = axs[2].imshow(N_direct_neighbor_per_matrix)\n",
    "    sc3 = axs[3].imshow(N_connect_total)\n",
    "    \n",
    "    cbar0 = fig.colorbar(sc0, ax=axs[0],extend='neither')\n",
    "    sc0.set_clim(vmin=0,vmax=1.1)\n",
    "\n",
    "    axs[1].set_xlim(0,Direct_neighbor.shape[0])\n",
    "    axs[1].set_ylim(0,Direct_neighbor.shape[0])\n",
    "    axs[1].invert_yaxis()\n",
    "    axs[1].set_aspect('equal')\n",
    "#     cbar1 = fig.colorbar(sc1, ax=axs[1],ticks=[0, 1])\n",
    "# #     cbar1.ax.set_yticklabels(['B', 'A'])\n",
    "#     cbar1.ax.set_yticklabels(['N','Y'])\n",
    "    \n",
    "    cbar2 = fig.colorbar(sc2, ax=axs[2],extend='neither')\n",
    "    sc2.set_clim(vmin=0,vmax=1)\n",
    "    \n",
    "    cbar3 = fig.colorbar(sc3, ax=axs[3],extend='neither')\n",
    "    sc3.set_clim(vmin=0,vmax=53)\n",
    "    \n",
    "    axs[2].xaxis.set_visible(False)\n",
    "    cbar0.set_label(r'Topological shortest distance ($\\mu m$)', fontsize=18)\n",
    "#     cbar1.set_label('Node type', fontsize=14)\n",
    "    axs[1].yaxis.set_label_position('right')\n",
    "    axs[1].set_ylabel('Direct Neighbors', fontsize=18)\n",
    "    cbar2.set_label('precentage of direct neighbors', fontsize=18)\n",
    "    cbar3.set_label('Number of nodes connecting path', fontsize=18)\n",
    "    axs[0].set_title(f't = {df.time[index]:.3f} s', fontsize=18)    \n",
    "    axs[3].set_title(f't = {df.time[index]:.3f} s', fontsize=18)    \n",
    "    axs[4].set_xlabel(\"Node\",fontsize=19)\n",
    "#     axs[4].set_ylabel(\"Dissimilarity\")\n",
    "#     plt.subplots_adjust(wspace = 0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot individual node clusters in 3d using coordinate values by plotting 5 or 6 largest clusters at each time points or tracked clusters from first 3 largest clusters in the first frame..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "R = [] #first cluster radius\n",
    "MIU2_XY = [] #normalized first central moment on x,y direction, suggesting deviation from circular shape (0 suggests circular)\n",
    "MIU2_XZ = [] #normalized first central moment on x,z direction\n",
    "MIU2_YZ = [] #normalized first central moment on y,z direction\n",
    "Cluster_size = [] #Number of nodes within the tracked cluster at each time frames\n",
    "# for index, row in tqdm(df.head(n=5).iterrows(), total=5):\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    g = row['graph']\n",
    "    Coord = g.vs['coordinate']\n",
    "    order = ORDER[index*g.vcount():(index+1)*g.vcount()]\n",
    "    n_connect_total = N_CONNECT_TOTAL[index*g.vcount():(index+1)*g.vcount()]\n",
    "    \n",
    "    ccs = g.clusters()\n",
    "    N_cluster = len(ccs) #N_cluster is the number of clusters within each frame\n",
    "    Ni_index = [] #Store the actual node index (not index directly from n_connect_total matrix) before clustering using order array\n",
    "    check_temp_cluster = 0 #Checkpoint for the row index between different squares in graph\n",
    "    if np.any(np.isnan(n_connect_total)): #whether N_CONNECT_TOTAL elements have been replace to NaN for plotting\n",
    "        for j in range(0,N_cluster):\n",
    "            temp_cluster = [i for i, e in enumerate(n_connect_total[check_temp_cluster]) if ~np.isnan(e)]\n",
    "            Ni_index.append(order[temp_cluster])\n",
    "            check_temp_cluster = check_temp_cluster+len(temp_cluster)\n",
    "    else:\n",
    "        for j in range(0,N_cluster):\n",
    "            temp_cluster = [i for i, e in enumerate(n_connect_total[check_temp_cluster]) if e!=0]\n",
    "            Ni_index.append(order[temp_cluster])\n",
    "            check_temp_cluster = check_temp_cluster+len(temp_cluster)\n",
    "\n",
    "    Len_Ni_index = [len(i) for i in Ni_index]\n",
    "    \n",
    "    #Find the cluster index that have the largest number of common node compared to the cluster in the previous frame, track 3 clusters        \n",
    "    if index !=0:\n",
    "        common_node_N1 = []\n",
    "        common_node_N2 = []\n",
    "        common_node_N3 = []\n",
    "        for i_node in Ni_index:\n",
    "            common_node_N1.append(len(set(N1_max_track_index_pre).intersection(set(i_node))))\n",
    "            common_node_N2.append(len(set(N2_max_track_index_pre).intersection(set(i_node))))\n",
    "            common_node_N3.append(len(set(N3_max_track_index_pre).intersection(set(i_node))))\n",
    "        max_index_N1 = common_node_N1.index(max(common_node_N1)) #node cluster index that has largest number of common nodes with cluster in the previous timeframe\n",
    "        max_index_N2 = common_node_N2.index(max(common_node_N2))\n",
    "        max_index_N3 = common_node_N3.index(max(common_node_N3))\n",
    "\n",
    "    #Find first five/six largest cluster at each time points\n",
    "    Sort_Len = np.argsort(Len_Ni_index) #Returns the indices that would sort the array in ascending order\n",
    "    N1_max_index = Ni_index[Sort_Len[len(Sort_Len)-1]]\n",
    "    N2_max_index = Ni_index[Sort_Len[len(Sort_Len)-2]]\n",
    "    N3_max_index = Ni_index[Sort_Len[len(Sort_Len)-3]]\n",
    "    N4_max_index = Ni_index[Sort_Len[len(Sort_Len)-4]]\n",
    "    N5_max_index = Ni_index[Sort_Len[len(Sort_Len)-5]]\n",
    "#     N6_max_index = Ni_index[Sort_Len[len(Sort_Len)-6]]\n",
    "\n",
    "    \n",
    "    #Define the connected cluster at this current time frame and is used for comparison at next time frame, track 3 clusters\n",
    "    if index == 0:\n",
    "        N1_max_track_index_pre = N1_max_index  #Choose the tracking cluster at first frame within range(0,N_cluster), here are the clusters among first five largest clusters in the first frame\n",
    "        N2_max_track_index_pre = N2_max_index\n",
    "        N3_max_track_index_pre = N3_max_index\n",
    "    else: \n",
    "        N1_max_track_index_pre = Ni_index[max_index_N1]\n",
    "        N2_max_track_index_pre = Ni_index[max_index_N2]\n",
    "        N3_max_track_index_pre = Ni_index[max_index_N3]\n",
    "    \n",
    "    Cluster_size.append(len(N1_max_track_index_pre))\n",
    "    \n",
    "    #Plot scatter plot for the first five/six largest clusters at each time points\n",
    "    fig = plt.figure(figsize=(14,5))\n",
    "    ax1 = fig.add_subplot(1,2,1, projection='3d')\n",
    "    N1_max_coord = [Coord[np.int(i)] for i in N1_max_index]\n",
    "    N2_max_coord = [Coord[np.int(i)] for i in N2_max_index]\n",
    "    N3_max_coord = [Coord[np.int(i)] for i in N3_max_index]\n",
    "    N4_max_coord = [Coord[np.int(i)] for i in N4_max_index]\n",
    "    N5_max_coord = [Coord[np.int(i)] for i in N5_max_index]  \n",
    "#     N6_max_coord = [Coord[np.int(i)] for i in N6_max_index]  \n",
    "    \n",
    "    miu1 = np.average(N1_max_coord,axis=0)\n",
    "    r_vector = N1_max_coord-miu1\n",
    "    r = np.median([np.sqrt(np.power(i,2).sum()) for i in r_vector]) #or max(...) or np.median(...) or np.average(...)\n",
    "    miu2_xy = sum([i[0]*i[1] for i in r_vector])/(len(r_vector)^2)\n",
    "    miu2_xz = sum([i[0]*i[2] for i in r_vector])/(len(r_vector)^2)\n",
    "    miu2_yz = sum([i[1]*i[2] for i in r_vector])/(len(r_vector)^2)\n",
    "    \n",
    "    R.append(r)\n",
    "    MIU2_XY.append(miu2_xy)\n",
    "    MIU2_XZ.append(miu2_xz)\n",
    "    MIU2_YZ.append(miu2_yz)\n",
    "    \n",
    "    ax1.scatter([i[0] for i in N1_max_coord], [i[1] for i in N1_max_coord], [i[2] for i in N1_max_coord], c='r', marker='o', alpha=0.7, s=8, linewidths=0)\n",
    "    ax1.scatter([i[0] for i in N2_max_coord], [i[1] for i in N2_max_coord], [i[2] for i in N2_max_coord], c='b', marker='o', alpha=0.7, s=8, linewidths=0)\n",
    "    ax1.scatter([i[0] for i in N3_max_coord], [i[1] for i in N3_max_coord], [i[2] for i in N3_max_coord], c='g', marker='o', alpha=0.7, s=8, linewidths=0)    \n",
    "    ax1.scatter([i[0] for i in N4_max_coord], [i[1] for i in N4_max_coord], [i[2] for i in N4_max_coord], c='c', marker='o', alpha=0.7, s=8, linewidths=0)    \n",
    "    ax1.scatter([i[0] for i in N5_max_coord], [i[1] for i in N5_max_coord], [i[2] for i in N5_max_coord], c='m', marker='o', alpha=0.7, s=8, linewidths=0)    \n",
    "#     ax1.scatter([i[0] for i in N6_max_coord], [i[1] for i in N6_max_coord], [i[2] for i in N6_max_coord], c='y', marker='o', alpha=0.7, s=8, linewidths=0)    \n",
    "   \n",
    "    ax1.set_title(f't = {df.time[index]:.3f} s')    \n",
    "\n",
    "    ticks = np.arange(-0.5, 0.5, 0.2)\n",
    "    ax1.set_xticks(ticks)\n",
    "    ax1.set_yticks(ticks)\n",
    "    ax1.set_zticks(ticks)\n",
    "\n",
    "    ax1.set_xlabel(r'Box X ($\\mu m$)')\n",
    "    ax1.set_ylabel(r'Box Y ($\\mu m$)')\n",
    "    ax1.set_zlabel(r'Box Z ($\\mu m$)')\n",
    "    \n",
    "    #Plot scatter plot for the tracked clusters (the three clusters in the 1st frame) over time \n",
    "    ax2 = fig.add_subplot(1,2,2, projection='3d')\n",
    "    connected_coord_N1 = [Coord[np.int(i)] for i in N1_max_track_index_pre]\n",
    "    connected_coord_N2 = [Coord[np.int(i)] for i in N2_max_track_index_pre]\n",
    "    connected_coord_N3 = [Coord[np.int(i)] for i in N3_max_track_index_pre]\n",
    "    ax2.scatter([i[0] for i in connected_coord_N1], [i[1] for i in connected_coord_N1], [i[2] for i in connected_coord_N1], c='r', marker='o', alpha=0.7, s=8, linewidths=0)\n",
    "    ax2.scatter([i[0] for i in connected_coord_N2], [i[1] for i in connected_coord_N2], [i[2] for i in connected_coord_N2], c='b', marker='o', alpha=0.7, s=8, linewidths=0)\n",
    "    ax2.scatter([i[0] for i in connected_coord_N3], [i[1] for i in connected_coord_N3], [i[2] for i in connected_coord_N3], c='g', marker='o', alpha=0.7, s=8, linewidths=0)\n",
    "    ax2.set_title(f't = {df.time[index]:.3f} s')\n",
    "\n",
    "    ticks = np.arange(-0.5, 0.5, 0.2)\n",
    "    ax2.set_xticks(ticks)\n",
    "    ax2.set_yticks(ticks)\n",
    "    ax2.set_zticks(ticks)\n",
    "\n",
    "    ax2.set_xlabel(r'Box X ($\\mu m$)')\n",
    "    ax2.set_ylabel(r'Box Y ($\\mu m$)')\n",
    "    ax2.set_zlabel(r'Box Z ($\\mu m$)')\n",
    "\n",
    "    plt.show()    \n",
    "\n",
    "#Plot 0th and 1st moment with time for the largest cluster, indicating its actual radius and circularity in x/y/z plane\n",
    "fig, axs = plt.subplots(1,2, figsize=(15,6))\n",
    "axs[0].plot(df.time,R)\n",
    "axs[0].set_ylabel(r'Radius of first cluster ($\\mu m$)', fontsize=18)\n",
    "axs[0].set_xlabel('Time (s)', fontsize=18)\n",
    "axs[0].set_ylim(0,0.18)\n",
    "\n",
    "axs[1].plot(df.time,MIU2_XY,color='blue', label='X,Y direction')\n",
    "axs[1].plot(df.time,MIU2_XZ,color='red', label='X,Z direction')\n",
    "axs[1].plot(df.time,MIU2_YZ,color='green', label='Y,Z direction')\n",
    "axs[1].set_ylabel('Normalized first central moment', fontsize=18)\n",
    "axs[1].set_xlabel('Time (s)', fontsize=18)\n",
    "axs[1].legend(loc='center left', bbox_to_anchor=(1, 0.9))\n",
    "axs[1].set_ylim(-0.012,0.012)\n",
    "\n",
    "#Plot the number of nodes within the first tracked cluster over time\n",
    "dt = np.diff(df.time)\n",
    "dCs = np.diff(Cluster_size)\n",
    "fig,axs = plt.subplots(1,2, figsize=(15,6))\n",
    "axs[0].plot(df.time,Cluster_size)\n",
    "axs[0].set_ylabel(r'Number of nodes within first tracked cluster', fontsize=18)\n",
    "axs[0].set_xlabel('Time (s)', fontsize=18)\n",
    "# ax[0].set_ylim(0,280)\n",
    "\n",
    "axs[1].plot(df.time[1:],[x/y for x, y in zip(dCs, dt)])\n",
    "axs[1].set_ylabel(r'First derivation of node number', fontsize=18)\n",
    "axs[1].set_xlabel('Time (s)', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot individual node clusters in 3d using coordinate values by plotting 5 or 6 largest clusters at each time points or tracked clusters from first 3 largest clusters in the last frame..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "R = [] #first cluster radius\n",
    "MIU2_XY = [] #normalized first central moment on x,y direction, suggesting deviation from circular shape (0 suggests circular)\n",
    "MIU2_XZ = [] #normalized first central moment on x,z direction\n",
    "MIU2_YZ = [] #normalized first central moment on y,z direction\n",
    "Cluster_size = [] #Number of nodes within the tracked cluster at each time frames\n",
    "\n",
    "df_copy = df.copy()  #Copy dataframe 'df' to a new dataframe 'df_copy' to avoid changing original dataframe\n",
    "reversed_df = df_copy.loc[::-1] #Reverse copied dataframe in terms of index and saved to 'reversed_df' dataframe\n",
    "# for index, row in tqdm(reversed_df.head(n=5).iterrows(), total=5):\n",
    "for index, row in tqdm(reversed_df.iterrows(), total=reversed_df.shape[0]):\n",
    "#     print(row.time, reversed_df.time[index], index, reversed_df.shape[0])\n",
    "    g = row['graph']\n",
    "    Coord = g.vs['coordinate']\n",
    "    order = ORDER[index*g.vcount():(index+1)*g.vcount()]\n",
    "    n_connect_total = N_CONNECT_TOTAL[index*g.vcount():(index+1)*g.vcount()]\n",
    "\n",
    "    \n",
    "    ccs = g.clusters()\n",
    "    N_cluster = len(ccs) #N_cluster is the number of clusters within each frame\n",
    "    Ni_index = [] #Store the actual node index (not index directly from n_connect_total matrix) before clustering using order array\n",
    "    check_temp_cluster = 0 #Checkpoint for the row index between different squares in graph\n",
    "    if np.any(np.isnan(n_connect_total)): #whether N_CONNECT_TOTAL elements have been replace to NaN for plotting\n",
    "        for j in range(0,N_cluster):\n",
    "            temp_cluster = [i for i, e in enumerate(n_connect_total[check_temp_cluster]) if ~np.isnan(e)]\n",
    "            Ni_index.append(order[temp_cluster])\n",
    "            check_temp_cluster = check_temp_cluster+len(temp_cluster)\n",
    "    else:\n",
    "        for j in range(0,N_cluster):\n",
    "            temp_cluster = [i for i, e in enumerate(n_connect_total[check_temp_cluster]) if e!=0]\n",
    "            Ni_index.append(order[temp_cluster])\n",
    "            check_temp_cluster = check_temp_cluster+len(temp_cluster)\n",
    "\n",
    "    Len_Ni_index = [len(i) for i in Ni_index]\n",
    "    \n",
    "    #Find the cluster index that have the largest number of common node compared to the cluster in the previous frame, track 3 clusters        \n",
    "    if index != reversed_df.shape[0]-1:\n",
    "        common_node_N1 = []\n",
    "        common_node_N2 = []\n",
    "        common_node_N3 = []\n",
    "        for i_node in Ni_index:\n",
    "            common_node_N1.append(len(set(N1_max_track_index_pre).intersection(set(i_node))))\n",
    "            common_node_N2.append(len(set(N2_max_track_index_pre).intersection(set(i_node))))\n",
    "            common_node_N3.append(len(set(N3_max_track_index_pre).intersection(set(i_node))))\n",
    "        max_index_N1 = common_node_N1.index(max(common_node_N1)) #node cluster index that has largest number of common nodes with cluster in the previous timeframe\n",
    "        max_index_N2 = common_node_N2.index(max(common_node_N2))\n",
    "        max_index_N3 = common_node_N3.index(max(common_node_N3))\n",
    "\n",
    "    #Find first five/six largest cluster at each time points\n",
    "    Sort_Len = np.argsort(Len_Ni_index) #Returns the indices that would sort the array in ascending order\n",
    "    N1_max_index = Ni_index[Sort_Len[len(Sort_Len)-1]]\n",
    "    N2_max_index = Ni_index[Sort_Len[len(Sort_Len)-2]]\n",
    "    N3_max_index = Ni_index[Sort_Len[len(Sort_Len)-3]]\n",
    "    N4_max_index = Ni_index[Sort_Len[len(Sort_Len)-4]]\n",
    "    N5_max_index = Ni_index[Sort_Len[len(Sort_Len)-5]]\n",
    "#     N6_max_index = Ni_index[Sort_Len[len(Sort_Len)-6]]\n",
    "\n",
    "    \n",
    "    #Define the connected cluster at this current time frame and is used for comparison at next time frame, track 3 clusters\n",
    "    if index == reversed_df.shape[0]-1:\n",
    "        N1_max_track_index_pre = N1_max_index  #Choose the tracking cluster at first frame within range(0,N_cluster), here are the clusters among first five largest clusters in the first frame\n",
    "        N2_max_track_index_pre = N2_max_index\n",
    "        N3_max_track_index_pre = N3_max_index\n",
    "    else: \n",
    "        N1_max_track_index_pre = Ni_index[max_index_N1]\n",
    "        N2_max_track_index_pre = Ni_index[max_index_N2]\n",
    "        N3_max_track_index_pre = Ni_index[max_index_N3]\n",
    "    \n",
    "    Cluster_size.append(len(N1_max_track_index_pre))\n",
    "    \n",
    "    #Plot scatter plot for the first five/six largest clusters at each time points\n",
    "    fig = plt.figure(figsize=(14,5))\n",
    "    ax1 = fig.add_subplot(1,2,1, projection='3d')\n",
    "    N1_max_coord = [Coord[np.int(i)] for i in N1_max_index]\n",
    "    N2_max_coord = [Coord[np.int(i)] for i in N2_max_index]\n",
    "    N3_max_coord = [Coord[np.int(i)] for i in N3_max_index]\n",
    "    N4_max_coord = [Coord[np.int(i)] for i in N4_max_index]\n",
    "    N5_max_coord = [Coord[np.int(i)] for i in N5_max_index]  \n",
    "#     N6_max_coord = [Coord[np.int(i)] for i in N6_max_index]  \n",
    "    \n",
    "    miu1 = np.average(N1_max_coord,axis=0)\n",
    "    r_vector = N1_max_coord-miu1\n",
    "    r = np.median([np.sqrt(np.power(i,2).sum()) for i in r_vector]) #or max(...) or np.median(...) or np.average(...)\n",
    "    miu2_xy = sum([i[0]*i[1] for i in r_vector])/(len(r_vector)^2)\n",
    "    miu2_xz = sum([i[0]*i[2] for i in r_vector])/(len(r_vector)^2)\n",
    "    miu2_yz = sum([i[1]*i[2] for i in r_vector])/(len(r_vector)^2)\n",
    "    \n",
    "    R.append(r)\n",
    "    MIU2_XY.append(miu2_xy)\n",
    "    MIU2_XZ.append(miu2_xz)\n",
    "    MIU2_YZ.append(miu2_yz)\n",
    "    \n",
    "    ax1.scatter([i[0] for i in N1_max_coord], [i[1] for i in N1_max_coord], [i[2] for i in N1_max_coord], c='r', marker='o', alpha=0.7, s=8, linewidths=0)\n",
    "    ax1.scatter([i[0] for i in N2_max_coord], [i[1] for i in N2_max_coord], [i[2] for i in N2_max_coord], c='b', marker='o', alpha=0.7, s=8, linewidths=0)\n",
    "    ax1.scatter([i[0] for i in N3_max_coord], [i[1] for i in N3_max_coord], [i[2] for i in N3_max_coord], c='g', marker='o', alpha=0.7, s=8, linewidths=0)    \n",
    "    ax1.scatter([i[0] for i in N4_max_coord], [i[1] for i in N4_max_coord], [i[2] for i in N4_max_coord], c='c', marker='o', alpha=0.7, s=8, linewidths=0)    \n",
    "    ax1.scatter([i[0] for i in N5_max_coord], [i[1] for i in N5_max_coord], [i[2] for i in N5_max_coord], c='m', marker='o', alpha=0.7, s=8, linewidths=0)    \n",
    "#     ax1.scatter([i[0] for i in N6_max_coord], [i[1] for i in N6_max_coord], [i[2] for i in N6_max_coord], c='y', marker='o', alpha=0.7, s=8, linewidths=0)    \n",
    "   \n",
    "    ax1.set_title(f't = {reversed_df.time[index]:.3f} s')    \n",
    "\n",
    "    ticks = np.arange(-0.5, 0.5, 0.2)\n",
    "    ax1.set_xticks(ticks)\n",
    "    ax1.set_yticks(ticks)\n",
    "    ax1.set_zticks(ticks)\n",
    "\n",
    "    ax1.set_xlabel(r'Box X ($\\mu m$)')\n",
    "    ax1.set_ylabel(r'Box Y ($\\mu m$)')\n",
    "    ax1.set_zlabel(r'Box Z ($\\mu m$)')\n",
    "    \n",
    "    #Plot scatter plot for the tracked clusters (the three clusters in the 1st frame) over time \n",
    "    ax2 = fig.add_subplot(1,2,2, projection='3d')\n",
    "    connected_coord_N1 = [Coord[np.int(i)] for i in N1_max_track_index_pre]\n",
    "    connected_coord_N2 = [Coord[np.int(i)] for i in N2_max_track_index_pre]\n",
    "    connected_coord_N3 = [Coord[np.int(i)] for i in N3_max_track_index_pre]\n",
    "    ax2.scatter([i[0] for i in connected_coord_N1], [i[1] for i in connected_coord_N1], [i[2] for i in connected_coord_N1], c='r', marker='o', alpha=0.7, s=8, linewidths=0)\n",
    "    ax2.scatter([i[0] for i in connected_coord_N2], [i[1] for i in connected_coord_N2], [i[2] for i in connected_coord_N2], c='b', marker='o', alpha=0.7, s=8, linewidths=0)\n",
    "    ax2.scatter([i[0] for i in connected_coord_N3], [i[1] for i in connected_coord_N3], [i[2] for i in connected_coord_N3], c='g', marker='o', alpha=0.7, s=8, linewidths=0)\n",
    "    ax2.set_title(f't = {reversed_df.time[index]:.3f} s')\n",
    "\n",
    "    ticks = np.arange(-0.5, 0.5, 0.2)\n",
    "    ax2.set_xticks(ticks)\n",
    "    ax2.set_yticks(ticks)\n",
    "    ax2.set_zticks(ticks)\n",
    "\n",
    "    ax2.set_xlabel(r'Box X ($\\mu m$)')\n",
    "    ax2.set_ylabel(r'Box Y ($\\mu m$)')\n",
    "    ax2.set_zlabel(r'Box Z ($\\mu m$)')\n",
    "\n",
    "    plt.show()    \n",
    "\n",
    "#Plot 0th and 1st moment with time for the largest cluster, indicating its actual radius and circularity in x/y/z plane\n",
    "fig, axs = plt.subplots(1,2, figsize=(15,6))\n",
    "axs[0].plot(reversed_df.time,R)\n",
    "axs[0].set_ylabel(r'Radius of first cluster ($\\mu m$)', fontsize=18)\n",
    "axs[0].set_xlabel('Time (s)', fontsize=18)\n",
    "axs[0].set_ylim(0,0.18)\n",
    "\n",
    "axs[1].plot(reversed_df.time,MIU2_XY,color='blue', label='X,Y direction')\n",
    "axs[1].plot(reversed_df.time,MIU2_XZ,color='red', label='X,Z direction')\n",
    "axs[1].plot(reversed_df.time,MIU2_YZ,color='green', label='Y,Z direction')\n",
    "axs[1].set_ylabel('Normalized first central moment', fontsize=18)\n",
    "axs[1].set_xlabel('Time (s)', fontsize=18)\n",
    "axs[1].legend(loc='center left', bbox_to_anchor=(1, 0.9))\n",
    "axs[1].set_ylim(-0.012,0.012)\n",
    "\n",
    "#Plot the number of nodes within the first tracked cluster over time\n",
    "dt = np.diff(reversed_df.time)\n",
    "dCs = np.diff(Cluster_size)\n",
    "fig,axs = plt.subplots(1,2, figsize=(15,6))\n",
    "axs[0].plot(reversed_df.time,Cluster_size)\n",
    "axs[0].set_ylabel(r'Number of nodes within first tracked cluster', fontsize=18)\n",
    "axs[0].set_xlabel('Time (s)', fontsize=18)\n",
    "# ax[0].set_ylim(0,280)\n",
    "\n",
    "axs[1].plot(reversed_df.time[1:],[x/y for x, y in zip(dCs, dt)])\n",
    "axs[1].set_ylabel(r'First derivation of node number', fontsize=18)\n",
    "axs[1].set_xlabel('Time (s)', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the neighbor properties of all nodes as well as the nodes within the largest cluster..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexA = np.array(range(N_A)) #Node index number for typeA molecules\n",
    "indexB = np.array(range(N_A,N_A+N_B)) #Node index number for typeB molecules\n",
    "\n",
    "N_neighbor_node = [] #For each node, number of neighbors at all time points\n",
    "N_neighbor_node_percen = [] #For each node, percentage of neighbors number relative to all available binding sites\n",
    "# node_label = [None]*len(N_neighbor_time[0]) #initialize the node_label with total number of A/B nodes\n",
    "for node_index in range(0,len(N_neighbor_time[0])):\n",
    "    N_neighbor_indi_node = [N_neighbor_time[time_index][node_index] for time_index in range(0,len(N_neighbor_time))]\n",
    "    #number of neighbors for each individual nodes at all time points\n",
    "    N_neighbor_node.append(N_neighbor_indi_node)\n",
    "    if node_index in indexA:\n",
    "#         node_label[node_index] = 'A'\n",
    "        N_neighbor_node_percen.append([x/6 for x in N_neighbor_indi_node])\n",
    "    else:\n",
    "#         node_label[node_index] = 'B'\n",
    "        N_neighbor_node_percen.append([x/2 for x in N_neighbor_indi_node])\n",
    "\n",
    "    \n",
    "N_neighbor_node_giant = [] #Initialize average neighbor numbers for nodes within largest cluster\n",
    "N_neighbor_node_giant_std = [] #std for neighbor numbers of nodes within largest cluster\n",
    "for time_index in range(0,len(giant_node)):\n",
    "    N_giant_node = giant_node[time_index]\n",
    "    N_neighbor_tempA = []\n",
    "    N_neighbor_tempB = []\n",
    "    for i in N_giant_node: \n",
    "        if i in indexA:\n",
    "            N_neighbor_tempA.append(N_neighbor_node[i][time_index])\n",
    "        else:\n",
    "            N_neighbor_tempB.append(N_neighbor_node[i][time_index])\n",
    "    N_neighbor_node_giant.append([np.average(N_neighbor_tempA),np.average(N_neighbor_tempB)])\n",
    "    N_neighbor_node_giant_std.append([np.std(N_neighbor_tempA),np.std(N_neighbor_tempB)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot number of neighbors for all nodes or nodes within largest cluster over time ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot number of neighbors for all nodes over time\n",
    "fig, ax = plt.subplots(1,1, figsize=(6,4))\n",
    "ax.set_ylim(0,6.5)\n",
    "ax.plot(df.time,np.average([N_neighbor_node[i] for i in indexA],axis=0), color='blue',label='hexamer neighbors')\n",
    "ax.plot(df.time,np.average([N_neighbor_node[i] for i in indexB],axis=0), color='red',label='dimer neighbors')\n",
    "ax.fill_between(\n",
    "    df.time,\n",
    "    y1 = np.average([N_neighbor_node[i] for i in indexA],axis=0)-np.std([N_neighbor_node[i] for i in indexA],axis=0),\n",
    "    y2 = np.average([N_neighbor_node[i] for i in indexA],axis=0)+np.std([N_neighbor_node[i] for i in indexA],axis=0), alpha=0.3, facecolor='gray')\n",
    "\n",
    "ax.fill_between(\n",
    "    df.time,\n",
    "    y1 = np.average([N_neighbor_node[i] for i in indexB],axis=0)-np.std([N_neighbor_node[i] for i in indexB],axis=0),\n",
    "    y2 = np.average([N_neighbor_node[i] for i in indexB],axis=0)+np.std([N_neighbor_node[i] for i in indexB],axis=0), alpha=0.3, color='gray')\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.9))\n",
    "ax.set_ylabel('Number of neighbors', fontsize=18)\n",
    "ax.set_xlabel('Time (s)', fontsize=18)\n",
    "plt.title('Average of all nodes')\n",
    "plt.show()\n",
    "\n",
    "# Plot number of neighbors for nodes within largest cluster over time\n",
    "fig, ax = plt.subplots(1,1, figsize=(6,4))\n",
    "ax.set_ylim(0,6.5)\n",
    "ax.plot(df.time,[N_neighbor_node_giant[i][0] for i in range(0,len(N_neighbor_node_giant))], color='blue',label='hexamer neighbors')\n",
    "ax.plot(df.time,[N_neighbor_node_giant[i][1] for i in range(0,len(N_neighbor_node_giant))], color='red',label='dimer neighbors')\n",
    "ax.fill_between(\n",
    "    df.time,\n",
    "    y1 = [N_neighbor_node_giant[i][0]-N_neighbor_node_giant_std[i][0] for i in range(0,len(N_neighbor_node_giant))],\n",
    "    y2 = [N_neighbor_node_giant[i][0]+N_neighbor_node_giant_std[i][0] for i in range(0,len(N_neighbor_node_giant))], alpha=0.3, facecolor='gray')\n",
    "\n",
    "ax.fill_between(\n",
    "    df.time,\n",
    "    y1 = [N_neighbor_node_giant[i][1]-N_neighbor_node_giant_std[i][1] for i in range(0,len(N_neighbor_node_giant))],\n",
    "    y2 = [N_neighbor_node_giant[i][1]+N_neighbor_node_giant_std[i][1] for i in range(0,len(N_neighbor_node_giant))], alpha=0.3, facecolor='gray')\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.9))\n",
    "ax.set_ylabel('Number of neighbors', fontsize=18)\n",
    "ax.set_xlabel('Time (s)', fontsize=18)\n",
    "plt.title('Average of the nodes within largest cluster')\n",
    "plt.show()\n",
    "\n",
    "# Plot autocorrelation of number of neighbors for all nodes over time\n",
    "fig, axes = plt.subplots(1,2, figsize=(12,4), sharey=True)\n",
    "# ax.set_ylim(0,6.5)\n",
    "\n",
    "if df.shape[0]>=100: # Defining the largest correlation length based on the size of the time points\n",
    "    MAXLAG = 100\n",
    "else:\n",
    "    MAXLAG = df.shape[0]-1\n",
    "    \n",
    "axes[0].acorr(np.average([N_neighbor_node[i] for i in indexA],axis=0), maxlags = MAXLAG, color='blue',label='hexamer neighbors')\n",
    "axes[1].acorr(np.average([N_neighbor_node[i] for i in indexB],axis=0), maxlags = MAXLAG, color='red',label='dimer neighbors') \n",
    "# axes.legend(loc='center left', bbox_to_anchor=(1, 0.9))\n",
    "axes[0].set_ylabel(f'Autocorrelation of \\n node neighbor numbers', fontsize=18)\n",
    "axes[0].set_xlabel('Time lag', fontsize=18)\n",
    "axes[1].set_xlabel('Time lag', fontsize=18)\n",
    "axes[0].set_title('Average of all hexamer neighbors')\n",
    "axes[1].set_title('Average of all dimer neighbors')\n",
    "plt.show()\n",
    "\n",
    "# Plot autocorrelation of number of neighbors of nodes within largest cluster in the last frame over time\n",
    "fig, axes = plt.subplots(1,2, figsize=(12,4), sharey=True)\n",
    "Final_giant_node = giant_node[-1]\n",
    "Final_giant_neighbor_A = [] #For each typeA node (hexamer) within last giant cluster, number of neighbors at all time points\n",
    "Final_giant_neighbor_B = [] #For each typeB node (dimer) within last giant cluster, number of neighbors at all time points\n",
    "for node_index in Final_giant_node:\n",
    "    if node_index in indexA:\n",
    "        Final_giant_neighbor_A.append(N_neighbor_node[node_index])\n",
    "    else:\n",
    "        Final_giant_neighbor_B.append(N_neighbor_node[node_index])\n",
    "axes[0].acorr(np.average(Final_giant_neighbor_A,axis=0), maxlags = MAXLAG, color='blue',label='hexamer neighbors')\n",
    "axes[1].acorr(np.average(Final_giant_neighbor_B,axis=0), maxlags = MAXLAG, color='red',label='dimer neighbors') \n",
    "# axes.legend(loc='center left', bbox_to_anchor=(1, 0.9))\n",
    "axes[0].set_ylabel(f'Autocorrelation of \\n node neighbor numbers \\n within last largest giant cluster', fontsize=18)\n",
    "axes[0].set_xlabel('Time lag', fontsize=18)\n",
    "axes[1].set_xlabel('Time lag', fontsize=18)\n",
    "axes[0].set_title(f'Average of hexamer neighbors \\n within last largest giant cluster')\n",
    "axes[1].set_title(f'Average of dimer neighbors \\n within last largest giant cluster')\n",
    "plt.show()\n",
    "\n",
    "# Plot colormap distribution of percentage neighbors and diffusivity for nodes within largest cluster\n",
    "cmap = plt.cm.get_cmap('jet')\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "# for index, row in tqdm(df.head(n=5).iterrows(), total=5):\n",
    "    g = row['graph']\n",
    "    Coord = g.vs['coordinate']\n",
    "    N_giant_node = giant_node[index]\n",
    "    N_giant_coord = [Coord[i] for i in N_giant_node]\n",
    "    N_giant_node_percen = [N_neighbor_node_percen[i][index] for i in N_giant_node]\n",
    "    N_giant_diffusion = [g.vs['diffusivity'][i] for i in N_giant_node]\n",
    "    \n",
    "    N_giant_node_A = []\n",
    "    N_giant_node_B = []\n",
    "    for i in range(0, len(N_giant_node)):\n",
    "        if N_giant_node[i] in indexA:\n",
    "            N_giant_node_A.append(N_giant_node[i])\n",
    "        else:\n",
    "            N_giant_node_B.append(N_giant_node[i])\n",
    "    N_giant_coord_A = [Coord[i] for i in N_giant_node_A]\n",
    "    N_giant_coord_B = [Coord[i] for i in N_giant_node_B]\n",
    "    N_giant_node_percen_A = [N_neighbor_node_percen[i][index] for i in N_giant_node_A]\n",
    "    N_giant_node_percen_B = [N_neighbor_node_percen[i][index] for i in N_giant_node_B]\n",
    "    N_giant_diffusion_A = [g.vs['diffusivity'][i] for i in N_giant_node_A]\n",
    "    N_giant_diffusion_B = [g.vs['diffusivity'][i] for i in N_giant_node_B]\n",
    "    \n",
    "    #First subplot - 3d node images\n",
    "    fig = plt.figure(figsize=(30,4))\n",
    "    gs = gridspec.GridSpec(1, 5, width_ratios=[1.5, 1.2, 1.2, 0.8, 0.8]) \n",
    "    ax3D = fig.add_subplot(gs[0], projection='3d')\n",
    "    sc3D = ax3D.scatter([i[0] for i in N_giant_coord], [i[1] for i in N_giant_coord], [i[2] for i in N_giant_coord], c=[i for i in N_giant_node_percen], cmap=cmap, marker='o', alpha=0.7, s=8, linewidths=0, depthshade=0)    \n",
    "    ax3D.set_title(f't = {df.time[index]:.3f} s')    \n",
    "    cbar = plt.colorbar(sc3D,extend='neither')\n",
    "    sc3D.set_clim(vmin=0,vmax=1)\n",
    "    cbar.set_label('percentage of direct neighbors', fontsize=14)\n",
    "    \n",
    "    def forceUpdate(event): #Solve the problem of point color changing in 3d scatter plot compared to 2d scatter plot\n",
    "        global sc3D\n",
    "        sc3D.changed()\n",
    "    fig.canvas.mpl_connect('draw_event', forceUpdate)\n",
    "\n",
    "    ax3D.set_xlim(-0.5,0.5)\n",
    "    ax3D.set_ylim(-0.5,0.5)\n",
    "    ax3D.set_zlim(-0.5,0.5)\n",
    "    ticks = np.arange(-0.5, 0.5, 0.2)\n",
    "    ax3D.set_xticks(ticks)\n",
    "    ax3D.set_yticks(ticks)\n",
    "    ax3D.set_zticks(ticks)\n",
    "\n",
    "    ax3D.set_xlabel(r'Box X ($\\mu m$)')\n",
    "    ax3D.set_ylabel(r'Box Y ($\\mu m$)')\n",
    "    ax3D.set_zlabel(r'Box Z ($\\mu m$)')\n",
    "    \n",
    "    #Second subplot - projection on x/y aixs node images with noder neighbor percentage as colorcode\n",
    "    ax = fig.add_subplot(gs[1])\n",
    "    sc = ax.scatter([i[0] for i in N_giant_coord], [i[1] for i in N_giant_coord], c=[i for i in N_giant_node_percen], cmap=cmap, marker='o', s=5, linewidths=0)    \n",
    "    ax.set_title(f't = {df.time[index]:.3f} s')    \n",
    "    cbar = plt.colorbar(sc,extend='neither')\n",
    "    sc.set_clim(vmin=0,vmax=1)\n",
    "    cbar.set_label('percentage of direct neighbors', fontsize=14)\n",
    "\n",
    "    ax.set_xlim(-0.5,0.5)\n",
    "    ax.set_ylim(-0.5,0.5)\n",
    "    ticks = np.arange(-0.5, 0.5, 0.2)\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_yticks(ticks)\n",
    "\n",
    "    ax.set_xlabel(r'Box X ($\\mu m$)')\n",
    "    ax.set_ylabel(r'Box Y ($\\mu m$)')\n",
    "\n",
    "    #Third subplot - projection on x/y aixs node images with diffusivity as colorcode\n",
    "    ax = fig.add_subplot(gs[2])\n",
    "    sc = ax.scatter([i[0] for i in N_giant_coord], [i[1] for i in N_giant_coord], c=[g.vs['diffusivity'][i]/(1e-12) for i in N_giant_node], cmap=cmap, marker='o', s=5, linewidths=0)    \n",
    "    ax.set_title(f't = {df.time[index]:.3f} s')    \n",
    "    cbar = plt.colorbar(sc,extend='neither')\n",
    "    sc.set_clim(vmin=0,vmax=1.0)\n",
    "    cbar.set_label(r'Avg. node diffusivity ($\\mu m^2/s$)', fontsize=14)\n",
    "\n",
    "    ax.set_xlim(-0.5,0.5)\n",
    "    ax.set_ylim(-0.5,0.5)\n",
    "    ticks = np.arange(-0.5, 0.5, 0.2)\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_yticks(ticks)\n",
    "\n",
    "    ax.set_xlabel(r'Box X ($\\mu m$)')\n",
    "    ax.set_ylabel(r'Box X ($\\mu m$)')\n",
    "    \n",
    "    #Forth subplot - A/B node separation projection on x/y aixs\n",
    "    ax = fig.add_subplot(gs[3])\n",
    "    sc = ax.scatter([i[0] for i in N_giant_coord_A], [i[1] for i in N_giant_coord_A], c='m', cmap=cmap, marker='o', label='A: Hexamer', s=5,alpha=0.5)    \n",
    "    sc = ax.scatter([i[0] for i in N_giant_coord_B], [i[1] for i in N_giant_coord_B], c='y', cmap=cmap, marker='o', label='B: Dimer', s=5, alpha=0.5)    \n",
    "    ax.set_title(f't = {df.time[index]:.3f} s')    \n",
    "\n",
    "#     ax.legend(loc='center left', bbox_to_anchor=(1, 0.9))\n",
    "    ax.set_xlim(-0.5,0.5)\n",
    "    ax.set_ylim(-0.5,0.5)\n",
    "    ticks = np.arange(-0.5, 0.5, 0.2)\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_yticks(ticks)\n",
    "\n",
    "    ax.set_xlabel(r'Box X ($\\mu m$)')\n",
    "    ax.set_ylabel(r'Box X ($\\mu m$)')\n",
    "\n",
    "    #Fifth subplot - Plots of number of neighbors for each node vs. node diffusivity\n",
    "    ax = fig.add_subplot(gs[4])\n",
    "    sc = ax.plot(N_giant_node_percen_A, [i/(1e-12) for i in N_giant_diffusion_A], 'mo', label='A: Hexamer',alpha=0.5)    \n",
    "    sc = ax.plot(N_giant_node_percen_B, [i/(1e-12) for i in N_giant_diffusion_B], 'yo', label='B: Dimer', alpha=0.5)    \n",
    "    ax.set_title(f't = {df.time[index]:.3f} s')    \n",
    "\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.9))\n",
    "    ax.set_ylim(-0.005,1.0)\n",
    "\n",
    "    ax.set_xlabel(r'Percentage of node neighbors')\n",
    "    ax.set_ylabel(r'Avg. node diffusivity ($\\mu m^2/s$)')\n",
    "    \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating topological properties of largest cluster..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['others_size'] = None\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        \n",
    "    g = row['graph']\n",
    "    \n",
    "    # Calculate connected compoents (CCs)\n",
    "    ccs = g.clusters()\n",
    "    \n",
    "    # Get giant component\n",
    "    giant = ccs.giant()\n",
    "\n",
    "    # Size of giant component\n",
    "    df.loc[index,'giant_size'] = giant.vcount()\n",
    "\n",
    "#     # Size of giant component in terms of #connections\n",
    "#     df.loc[index,'giant_size'] = giant.ecount()\n",
    "    \n",
    "    # Diameter of giant connected component\n",
    "    # Diameters is defined as the longest shortest path between two pairs of nodes\n",
    "#     df.loc[index,'giant_diameter'] = giant.diameter()\n",
    "    df.loc[index,'giant_diameter'] = giant.diameter(weights=giant.es['length'])\n",
    "    \n",
    "    # Std and mean of sizes of remaining components\n",
    "    sizes = ccs.sizes()  # This is equivalent to  [g.vcount() for g in ccs.subgraphs()]\n",
    "#     sizes = [g.ecount() for g in ccs.subgraphs()]\n",
    "\n",
    "    # Exclude giant cc\n",
    "    sizes.pop(sizes.index(giant.vcount()))\n",
    "#     sizes.pop(sizes.index(giant.ecount()))\n",
    "    df.loc[index,'mean_others_size'] = np.mean(sizes) if sizes else None\n",
    "    df.loc[index,'std_others_size'] = np.std(sizes) if sizes else None\n",
    "    \n",
    "    # Frequency of clusters with size 0, 1, 2..\n",
    "    hist = np.bincount(sizes)\n",
    "    df.at[index,'others_size'] = hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ymax = 1.1*df.giant_size.max()\n",
    "fig, ax1 = plt.subplots(1,1, figsize=(6,4))\n",
    "ax1.plot(df.time[:77],df.giant_size[:77], color='blue', label='Number of nodes within largest cluster')\n",
    "ax1.set_ylabel(\"Number of nodes \\n within largest cluster\", fontsize=18, color='blue')\n",
    "ax1.set_xlabel('Time (s)', fontsize=18)\n",
    "# ax1.set_ylim(0,280)\n",
    "ax1.set_ylim(0,250)\n",
    "plt.show()\n",
    "\n",
    "fig, ax2 = plt.subplots(1,1, figsize=(6,4))\n",
    "ax2.set_ylabel('Topological diameter \\n of largest cluster (Âµm)', fontsize=18, color='red')\n",
    "ax2.plot(df.time[1:78],df.giant_diameter[:77], color='red', label='Topological diameter of largest cluster')\n",
    "ax2.set_ylim(0,1.2)\n",
    "# ax1.legend(loc='center left', bbox_to_anchor=(1.15, 0.9))\n",
    "# ax2.legend(loc='center left', bbox_to_anchor=(1.15, 0.8))\n",
    "plt.show\n",
    "# fig.savefig('./Ensemble-node-links/box_Viscosity_trail19_(1)_nodelinks/giant cluster trail19 (1).pdf')\n",
    "\n",
    "#Plot first derivative of nodes number within largest cluster vs. time\n",
    "dt = np.diff(df.time[:77])\n",
    "dGDs = np.diff(df.giant_size[:77])\n",
    "fig, ax = plt.subplots(1,1, figsize=(6,4))\n",
    "ax.plot(df.time[1:77],[x/y for x, y in zip(dGDs, dt)])\n",
    "ax.set_ylabel(\"First derivative of node number \\n within largest cluster\", fontsize=18)\n",
    "ax.set_xlabel('Time (s)', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dataframe with time and file paths for links and coordinates..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# xmax = df.time.max()\n",
    "xmax = 0.77\n",
    "ymax = 1.1*df.giant_size.max()\n",
    "fig, ax = plt.subplots(1,1, figsize=(6,4))\n",
    "ax.set_ylabel('Size of remaining CCs', fontsize=18)\n",
    "ax.set_xlabel('Time (s)', fontsize=18)\n",
    "ax.set_xlim(0,xmax)\n",
    "ax.set_ylim(0,30)\n",
    "df_sub = df.dropna()\n",
    "ax.fill_between(\n",
    "    df_sub.time,\n",
    "    y1 = df_sub.mean_others_size-df_sub.std_others_size,\n",
    "    y2 = df_sub.mean_others_size+df_sub.std_others_size, alpha=0.3, color='gray')\n",
    "ax.plot(df_sub.time,df_sub.mean_others_size, '-o', color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Evolution of clusters sizes as a heatmap\n",
    "# This does not include the giant cluster\n",
    "\n",
    "# Calculate the maximum number of co-occurent clusters\n",
    "nmax = []\n",
    "for index in df.index[:77]:\n",
    "    hist = df.others_size[index]\n",
    "    if hist.size:\n",
    "        nmax.append(len(hist))\n",
    "# Empty heatmap\n",
    "data = np.zeros((np.max(nmax),df.shape[0]), dtype=np.uint64)\n",
    "# Fill heatmap in\n",
    "for i, index in enumerate(df.index[:77]):\n",
    "    hist = df.others_size[index]\n",
    "    data[:len(hist),i] = hist\n",
    "fig, ax = plt.subplots(1,1, figsize=(12,4))\n",
    "sc0 = ax.imshow(np.log(1+data[:40,:77]),cmap='jet')\n",
    "# sc0 = ax.imshow(data,cmap='jet')\n",
    "# ax.set_xlabel(f'Time (s x {df.shape[0]})', fontsize=18)\n",
    "# ax.set_xlabel('Time (s x %1.2f)' %np.diff(df_sub.time)[0], fontsize=18)\n",
    "ax.set_xlabel(f'Time (x {np.diff(df_sub.time)[0]:.3f} s)', fontsize=18)\n",
    "ax.set_ylabel('Cluster size', fontsize=18)\n",
    "clbar = fig.colorbar(sc0,extend='neither')\n",
    "sc0.set_clim(vmin=0,vmax=6.8)\n",
    "clbar.set_label('ln(1 + #Clusters)',fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fraction of neighboors that remains the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idxi, idxj in tqdm(zip(df.index[:-1],df.index[1:]), total=df.shape[0]-1):\n",
    "    \n",
    "    # Load graph at time   t: gi\n",
    "    # Load graph at time t+1: gj\n",
    "    gi = df.graph[idxi]\n",
    "    gj = df.graph[idxj]\n",
    "    \n",
    "    # Get list of neighbors\n",
    "    neighi = gi.neighborhood()\n",
    "    neighj = gj.neighborhood()\n",
    "        \n",
    "    fraction = []\n",
    "    \n",
    "    # Check the fraction of nodes with unchanged neighborhood\n",
    "    for nik, njk in zip(neighi,neighj):\n",
    "        \n",
    "        # Intersection between the two sets of neighs\n",
    "        common = set(njk).intersection(set(nik))\n",
    "        \n",
    "        fraction.append(len(common) / np.max([len(nik),len(njk)]))\n",
    "        \n",
    "    df.loc[idxj,'frac_sim_neighs_avg'] = np.mean(fraction)\n",
    "    df.loc[idxj,'frac_sim_neighs_std'] =  np.std(fraction)\n",
    "\n",
    "# Plot results\n",
    "# xmax = df.time.max()\n",
    "xmax = 0.77\n",
    "fig, ax = plt.subplots(1,1, figsize=(6,4))\n",
    "ax.set_ylabel('Fraction of unchanged neighbors', fontsize=18)\n",
    "ax.set_xlabel('Time (s)', fontsize=18)\n",
    "ax.set_xlim(0,xmax)\n",
    "ax.set_ylim(0.6,1.1)\n",
    "ax.fill_between(\n",
    "    df.time,\n",
    "    y1 = df.frac_sim_neighs_avg-df.frac_sim_neighs_std,\n",
    "    y2 = df.frac_sim_neighs_avg+df.frac_sim_neighs_std, alpha=0.3, color='gray')\n",
    "ax.plot(df.time,df.frac_sim_neighs_avg, '-o', color='black')\n",
    "\n",
    "# Plot zoom-in results from 0-0.25s\n",
    "fig, ax_zoom = plt.subplots(1,1, figsize=(1,4))\n",
    "# ax_zoom.set_ylabel('Fraction of unchanged neighbors', fontsize=18)\n",
    "ax_zoom.set_xlabel('Time (s)', fontsize=18)\n",
    "ax_zoom.set_xlim(0,0.25)\n",
    "ax_zoom.set_ylim(0.6,1.1)\n",
    "ax_zoom.fill_between(\n",
    "    df.time[0:25],\n",
    "    y1 = df.frac_sim_neighs_avg[0:25]-df.frac_sim_neighs_std[0:25],\n",
    "    y2 = df.frac_sim_neighs_avg[0:25]+df.frac_sim_neighs_std[0:25], alpha=0.3, color='gray')\n",
    "ax_zoom.plot(df.time[0:25],df.frac_sim_neighs_avg[0:25], '-o', color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time aggregated graph and hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Number of graphs to be aggregated together\n",
    "n_agg_graphs = 3 # how many time frames (in the selected nodelink files) are used for each aggregation, not second in unit\n",
    "\n",
    "times = df['time'].values\n",
    "tstep = np.median(np.diff(df.time.values))\n",
    "nbins = np.int(np.round(len(times)/n_agg_graphs))+1\n",
    "\n",
    "# nbins = np.int(np.round((times.max()-times.min())/(n_agg_graphs*tstep)))+1\n",
    "# df['agg_time'] = np.digitize(\n",
    "#     times,\n",
    "#     np.linspace(times.min(),times.max(),nbins)\n",
    "# )\n",
    "\n",
    "df['agg_index'] = np.digitize(\n",
    "    range(0,len(times)),\n",
    "    np.linspace(0,len(times),nbins)\n",
    ")\n",
    "# print(df['agg_index'])\n",
    "# print(np.linspace(0,len(times),nbins))\n",
    "# print([[agg_time] for agg_time, df_agg in df.groupby('agg_index')])\n",
    "# print(len([[agg_time] for agg_time, df_agg in df.groupby('agg_index')]))\n",
    "\n",
    "for agg_time, df_agg in df.groupby('agg_index'):\n",
    "\n",
    "    # Get first adjacency matrix\n",
    "    adj = df_agg.graph[df_agg.index[0]].get_adjacency(attribute='length')\n",
    "    \n",
    "    for g in df_agg.graph.values[1:]:\n",
    "        adj += g.get_adjacency(attribute='length')\n",
    "        \n",
    "    adj = np.array(adj.data).reshape(adj.shape)\n",
    "        \n",
    "    # Min and max in the weighted adj matrix\n",
    "    dmax = adj.max()\n",
    "    dmin = adj[adj>0].min()\n",
    "        \n",
    "    # Neighbors more often connected have low weight\n",
    "    adj[adj>0] = np.abs( adj[adj>0] - (dmin+dmax) ).astype(np.float)\n",
    "    adj = adj / n_agg_graphs\n",
    "    \n",
    "    # Create a graph from the aggregated adj matrix\n",
    "    g_agg = igraph.Graph.Adjacency((adj > 0).tolist(), mode=igraph.ADJ_UNDIRECTED)\n",
    "    g_agg.es['length'] = adj[adj.nonzero()]\n",
    "    \n",
    "    # Calculate shortest paths length\n",
    "    spl = g_agg.shortest_paths_dijkstra(weights=g_agg.es['length'])\n",
    "    distances = np.array(spl).reshape(adj.shape)\n",
    "\n",
    "    # Replace infinities with a very large distance\n",
    "    distances[np.isinf(distances)] = distances[~np.isinf(distances)].max()\n",
    "    \n",
    "    # Clustering\n",
    "    threshold = 0.3\n",
    "    fig, axs = plt.subplots(1,2,figsize=(12,4))\n",
    "    linkage = hierarchy.linkage(distances, method=\"single\")\n",
    "    clusters = hierarchy.fcluster(linkage, threshold, criterion=\"distance\")\n",
    "    dend = hierarchy.dendrogram(linkage, color_threshold=threshold, ax=axs[1])\n",
    "    order = dend['leaves']\n",
    "    distances = distances[order,:]\n",
    "    distances = distances[:,order]\n",
    "    \n",
    "    #After clustering, convert the largest distances (& not indirectly connected nodes) to nan and plot as white in the colormap\n",
    "    max_index2 = np.where(distances == np.amax(distances)) \n",
    "    distances[max_index2] = np.nan\n",
    "    current_cmap = plt.cm.get_cmap()\n",
    "    current_cmap.set_bad(color='white')\n",
    " \n",
    "    sc1 = axs[0].imshow(distances)\n",
    "#     cbar = fig.colorbar(sc1,ax=axs[0])\n",
    "    cbar = fig.colorbar(sc1, ax=axs[0],extend='neither')\n",
    "    sc1.set_clim(vmin=0,vmax=0.9)\n",
    "    \n",
    "    cbar.set_label(r'Adjusted frame-wise distance ($\\mu m$)', fontsize=14)\n",
    "    axs[0].set_title(f'Aggregate: {df_agg.time.min():.3f} to {df_agg.time.max():.3f} s')    \n",
    "    axs[1].set_xlabel(\"Node\")\n",
    "#     axs[1].set_ylabel(\"Dissimilarity\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diffusivity as a function of cluster size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff = pd.DataFrame([])\n",
    "\n",
    "for index in tqdm(df.index):\n",
    "    \n",
    "    g = df.graph[index]\n",
    "    \n",
    "    ccs = g.clusters()\n",
    "    \n",
    "    df_cc = pd.DataFrame([{'cluster': m, 'diffusivity': d} for (m,d) in zip(ccs.membership,g.vs['diffusivity'])])\n",
    "    sizes = df_cc.groupby('cluster').size()\n",
    "    df_cc = df_cc.groupby('cluster').agg(['mean','std'])\n",
    "    df_cc['time'] = df.time[index]\n",
    "    df_cc['nmols'] = sizes\n",
    "#     print(sum(sizes))\n",
    "    df_cc.head()\n",
    "    \n",
    "    df_diff = pd.concat([df_diff,df_cc], axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cmap = plt.cm.get_cmap('jet')\n",
    "fig, ax = plt.subplots(1,1, figsize=(6,4))\n",
    "# sc = ax.scatter(df_diff.nmols,df_diff[('diffusivity','mean')]/(1e-13), s=1, c=df_diff.time, cmap=cmap)\n",
    "# timemax = max(df.time)\n",
    "# timemax = max(df.time)\n",
    "timemax = 0.77\n",
    "timemin = max(df.time)*0\n",
    "timeindex = df_diff.time[(df_diff.time >= timemin) & (df_diff.time <= timemax)].index\n",
    "sc = ax.scatter(df_diff.nmols[timeindex],df_diff[('diffusivity','mean')][timeindex]/(1e-12), s=1,c=df_diff.time[timeindex], cmap=cmap)\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('Cluster size (#molecules)', fontsize=18)\n",
    "ax.set_ylabel(r'Avg. diffusivity ($\\mu m^2/s$)', fontsize=18)\n",
    "# ax.set_ylabel(r'Avg. diffusivity ($\\times 10^{-13}~m^2/s$)', fontsize=18)\n",
    "ax.set_xlim(0.75,300)\n",
    "ax.set_ylim(0.18,1.3)\n",
    "# ax.set_ylim(0.25,1.3)\n",
    "cbar = plt.colorbar(sc,extend='neither')\n",
    "sc.set_clim(vmin=timemin,vmax=timemax)\n",
    "cbar.set_label('time (s)', fontsize=14)\n",
    "\n",
    "# Plot the average molecular diffusion constants vs the cluster size that molecules are within\n",
    "fig2, ax2 = plt.subplots(1,1, figsize=(6,4))\n",
    "df_diff_timeindex = pd.DataFrame({\"cluster_size\": df_diff.nmols[timeindex], \"Diff_cluster_size\": df_diff[('diffusivity','mean')][timeindex]})\n",
    "Diff_cluster_size = df_diff_timeindex.groupby('cluster_size').agg(['mean','std'])\n",
    "Diff_cluster_size = Diff_cluster_size.reset_index() #Reset the dataframe to regular column\n",
    "# Linear fitting the log scale graph (check power law index)\n",
    "coef = np.polyfit(np.log10(Diff_cluster_size[\"cluster_size\"]),np.log10(Diff_cluster_size[('Diff_cluster_size','mean')]),1)\n",
    "poly1d_fn = np.poly1d(coef) # poly1d_fn is now a function which takes in x and returns an estimate for y\n",
    "Diff_cluster_size_fit = [pow(10,i)/(1e-12) for i in poly1d_fn(np.log10(Diff_cluster_size[\"cluster_size\"]))]\n",
    "plt.errorbar(Diff_cluster_size[\"cluster_size\"], Diff_cluster_size[('Diff_cluster_size','mean')]/(1e-12), yerr = Diff_cluster_size[('Diff_cluster_size','std')]/(1e-12), xerr = None)\n",
    "ax2.plot(Diff_cluster_size[\"cluster_size\"],Diff_cluster_size_fit,'--r')\n",
    "ax2.text(50, 0.5, f'Exponent $\\\\alpha$={round(coef[0], 2)}')\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_yscale('log')\n",
    "ax2.set_xlabel('Cluster size (#molecules)', fontsize=18)\n",
    "ax2.set_ylabel(r'Avg. diffusivity ($\\mu m^2/s$)', fontsize=18)\n",
    "\n",
    "# Plot the average cluster size vs time\n",
    "fig3, ax3 = plt.subplots(1,1, figsize=(6,4))\n",
    "cluster_timeindex = pd.DataFrame({\"time\": df_diff.time[timeindex],\"cluster_size\": df_diff.nmols[timeindex]})\n",
    "cluster_time = cluster_timeindex.groupby(\"time\").agg(['mean','std'])\n",
    "cluster_time = cluster_time.reset_index() #Reset the dataframe to regular column\n",
    "# Linear fitting the log scale graph (check power law index)\n",
    "coef = np.polyfit(np.log10(cluster_time[\"time\"]),np.log10(cluster_time[('cluster_size','mean')]),1)\n",
    "poly1d_fn = np.poly1d(coef) # poly1d_fn is now a function which takes in x and returns an estimate for y\n",
    "cluster_time_fit = [pow(10,i) for i in poly1d_fn(np.log10(cluster_time[\"time\"]))]\n",
    "plt.errorbar(cluster_time[\"time\"], cluster_time[('cluster_size','mean')], yerr = cluster_time[('cluster_size','std')], xerr = None)\n",
    "ax3.plot(cluster_time['time'],cluster_time_fit,'--r')\n",
    "ax3.text(0.01, 20, f'Exponent $\\\\alpha$={round(coef[0], 2)}')\n",
    "# ax3.set_ylim([1,35])\n",
    "ax3.set_xscale('log')\n",
    "ax3.set_yscale('log')\n",
    "ax3.set_xlabel('time / s', fontsize=18)\n",
    "ax3.set_ylabel(r'Avg. cluster size (#molecules)', fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.cm.get_cmap('jet')\n",
    "fig, ax = plt.subplots(1,1, figsize=(6,4))\n",
    "# sc = ax.scatter(df_diff.nmols,df_diff[('diffusivity','mean')]/(1e-13), s=1, c=df_diff.time, cmap=cmap)\n",
    "# timemax = max(df.time)\n",
    "# timemax = max(df.time)\n",
    "timemax = 0.2\n",
    "timemin = max(df.time)*0\n",
    "timeindex = df_diff.time[(df_diff.time >= timemin) & (df_diff.time <= timemax)].index\n",
    "sc = ax.scatter(df_diff.nmols[timeindex],df_diff[('diffusivity','mean')][timeindex]/(1e-12), s=1,c=df_diff.time[timeindex], cmap=cmap)\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('Cluster size (#molecules)', fontsize=18)\n",
    "ax.set_ylabel(r'Avg. diffusivity ($\\mu m^2/s$)', fontsize=18)\n",
    "# ax.set_ylabel(r'Avg. diffusivity ($\\times 10^{-13}~m^2/s$)', fontsize=18)\n",
    "ax.set_xlim(0.75,300)\n",
    "ax.set_ylim(0.18,1.3)\n",
    "# ax.set_ylim(0.25,1.3)\n",
    "cbar = plt.colorbar(sc,extend='neither')\n",
    "sc.set_clim(vmin=timemin,vmax=timemax)\n",
    "cbar.set_label('time (s)', fontsize=14)\n",
    "\n",
    "# Plot the average molecular diffusion constants vs the cluster size that molecules are within\n",
    "fig2, ax2 = plt.subplots(1,1, figsize=(6,4))\n",
    "df_diff_timeindex = pd.DataFrame({\"cluster_size\": df_diff.nmols[timeindex], \"Diff_cluster_size\": df_diff[('diffusivity','mean')][timeindex]})\n",
    "Diff_cluster_size = df_diff_timeindex.groupby('cluster_size').agg(['mean','std'])\n",
    "Diff_cluster_size = Diff_cluster_size.reset_index() #Reset the dataframe to regular column\n",
    "# Linear fitting the log scale graph (check power law index)\n",
    "coef = np.polyfit(np.log10(Diff_cluster_size[\"cluster_size\"]),np.log10(Diff_cluster_size[('Diff_cluster_size','mean')]),1)\n",
    "poly1d_fn = np.poly1d(coef) # poly1d_fn is now a function which takes in x and returns an estimate for y\n",
    "Diff_cluster_size_fit = [pow(10,i)/(1e-12) for i in poly1d_fn(np.log10(Diff_cluster_size[\"cluster_size\"]))]\n",
    "plt.errorbar(Diff_cluster_size[\"cluster_size\"], Diff_cluster_size[('Diff_cluster_size','mean')]/(1e-12), yerr = Diff_cluster_size[('Diff_cluster_size','std')]/(1e-12), xerr = None)\n",
    "ax2.plot(Diff_cluster_size[\"cluster_size\"],Diff_cluster_size_fit,'--r')\n",
    "ax2.text(250, 0.5, f'Exponent $\\\\alpha$={round(coef[0], 2)}')\n",
    "ax2.set_ylim([0.22,0.7])\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_yscale('log')\n",
    "ax2.set_xlabel('Cluster size (#molecules)', fontsize=18)\n",
    "ax2.set_ylabel(r'Avg. diffusivity ($\\mu m^2/s$)', fontsize=18)\n",
    "\n",
    "# Plot the average cluster size vs time\n",
    "fig3, ax3 = plt.subplots(1,1, figsize=(6,4))\n",
    "cluster_timeindex = pd.DataFrame({\"time\": df_diff.time[timeindex],\"cluster_size\": df_diff.nmols[timeindex]})\n",
    "cluster_time = cluster_timeindex.groupby(\"time\").agg(['mean','std'])\n",
    "cluster_time = cluster_time.reset_index() #Reset the dataframe to regular column\n",
    "# Linear fitting the log scale graph (check power law index)\n",
    "coef = np.polyfit(np.log10(cluster_time[\"time\"]),np.log10(cluster_time[('cluster_size','mean')]),1)\n",
    "poly1d_fn = np.poly1d(coef) # poly1d_fn is now a function which takes in x and returns an estimate for y\n",
    "cluster_time_fit = [pow(10,i) for i in poly1d_fn(np.log10(cluster_time[\"time\"]))]\n",
    "plt.errorbar(cluster_time[\"time\"], cluster_time[('cluster_size','mean')], yerr = cluster_time[('cluster_size','std')], xerr = None)\n",
    "ax3.plot(cluster_time['time'],cluster_time_fit,'--r')\n",
    "ax3.text(0.01, 25, f'Exponent $\\\\alpha$={round(coef[0], 2)}')\n",
    "ax3.set_ylim([1,20])\n",
    "ax3.set_xscale('log')\n",
    "ax3.set_yscale('log')\n",
    "ax3.set_xlabel('time / s', fontsize=18)\n",
    "ax3.set_ylabel(r'Avg. cluster size (#molecules)', fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.cm.get_cmap('jet')\n",
    "fig, ax = plt.subplots(1,1, figsize=(6,4))\n",
    "# sc = ax.scatter(df_diff.nmols,df_diff[('diffusivity','mean')]/(1e-13), s=1, c=df_diff.time, cmap=cmap)\n",
    "# timemax = max(df.time)\n",
    "# timemax = max(df.time)\n",
    "timemax = 0.77\n",
    "# timemin = max(df.time)*0\n",
    "timemin = 0.2\n",
    "timeindex = df_diff.time[(df_diff.time >= timemin) & (df_diff.time <= timemax)].index\n",
    "sc = ax.scatter(df_diff.nmols[timeindex],df_diff[('diffusivity','mean')][timeindex]/(1e-12), s=1,c=df_diff.time[timeindex], cmap=cmap)\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('Cluster size (#molecules)', fontsize=18)\n",
    "ax.set_ylabel(r'Avg. diffusivity ($\\mu m^2/s$)', fontsize=18)\n",
    "# ax.set_ylabel(r'Avg. diffusivity ($\\times 10^{-13}~m^2/s$)', fontsize=18)\n",
    "ax.set_xlim(0.75,300)\n",
    "ax.set_ylim(0.18,1.3)\n",
    "# ax.set_ylim(0.25,1.3)\n",
    "cbar = plt.colorbar(sc,extend='neither')\n",
    "sc.set_clim(vmin=timemin,vmax=timemax)\n",
    "cbar.set_label('time (s)', fontsize=14)\n",
    "\n",
    "# Plot the average molecular diffusion constants vs the cluster size that molecules are within\n",
    "fig2, ax2 = plt.subplots(1,1, figsize=(6,4))\n",
    "df_diff_timeindex = pd.DataFrame({\"cluster_size\": df_diff.nmols[timeindex], \"Diff_cluster_size\": df_diff[('diffusivity','mean')][timeindex]})\n",
    "Diff_cluster_size = df_diff_timeindex.groupby('cluster_size').agg(['mean','std'])\n",
    "Diff_cluster_size = Diff_cluster_size.reset_index() #Reset the dataframe to regular column\n",
    "# Linear fitting the log scale graph (check power law index)\n",
    "coef = np.polyfit(np.log10(Diff_cluster_size[\"cluster_size\"]),np.log10(Diff_cluster_size[('Diff_cluster_size','mean')]),1)\n",
    "poly1d_fn = np.poly1d(coef) # poly1d_fn is now a function which takes in x and returns an estimate for y\n",
    "Diff_cluster_size_fit = [pow(10,i)/(1e-12) for i in poly1d_fn(np.log10(Diff_cluster_size[\"cluster_size\"]))]\n",
    "plt.errorbar(Diff_cluster_size[\"cluster_size\"], Diff_cluster_size[('Diff_cluster_size','mean')]/(1e-12), yerr = Diff_cluster_size[('Diff_cluster_size','std')]/(1e-12), xerr = None)\n",
    "ax2.plot(Diff_cluster_size[\"cluster_size\"],Diff_cluster_size_fit,'--r')\n",
    "ax2.text(250, 0.5, f'Exponent $\\\\alpha$={round(coef[0], 2)}')\n",
    "ax2.set_ylim([0.22,0.7])\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_yscale('log')\n",
    "ax2.set_xlabel('Cluster size (#molecules)', fontsize=18)\n",
    "ax2.set_ylabel(r'Avg. diffusivity ($\\mu m^2/s$)', fontsize=18)\n",
    "\n",
    "# Plot the average cluster size vs time\n",
    "fig3, ax3 = plt.subplots(1,1, figsize=(6,4))\n",
    "cluster_timeindex = pd.DataFrame({\"time\": df_diff.time[timeindex],\"cluster_size\": df_diff.nmols[timeindex]})\n",
    "cluster_time = cluster_timeindex.groupby(\"time\").agg(['mean','std'])\n",
    "cluster_time = cluster_time.reset_index() #Reset the dataframe to regular column\n",
    "# Linear fitting the log scale graph (check power law index)\n",
    "coef = np.polyfit(np.log10(cluster_time[\"time\"]),np.log10(cluster_time[('cluster_size','mean')]),1)\n",
    "poly1d_fn = np.poly1d(coef) # poly1d_fn is now a function which takes in x and returns an estimate for y\n",
    "cluster_time_fit = [pow(10,i) for i in poly1d_fn(np.log10(cluster_time[\"time\"]))]\n",
    "plt.errorbar(cluster_time[\"time\"], cluster_time[('cluster_size','mean')], yerr = cluster_time[('cluster_size','std')], xerr = None)\n",
    "ax3.plot(cluster_time['time'],cluster_time_fit,'--r')\n",
    "ax3.text(0.25, 35, f'Exponent $\\\\alpha$={round(coef[0], 2)}')\n",
    "ax3.set_ylim([2,30])\n",
    "ax3.set_xscale('log')\n",
    "ax3.set_yscale('log')\n",
    "ax3.set_xlabel('time / s', fontsize=18)\n",
    "ax3.set_ylabel(r'Avg. cluster size (#molecules)', fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replot figures for the paper:\n",
    "\n",
    "* Make figures vectorized \n",
    "* Graph represented cluster at t = 0.01s, 0.25s, 0.50s, 0.77s by rescale the colorbar\n",
    "* Overlay the histogram of cluster size distribution at the above time points t = 0.01s, 0.25s, 0.50s, 0.77s\n",
    "* Time course of tracking the largest cluster at the last time point\n",
    "* Replot the cluster diffusivity with cluster size by combining all time information and use single color for each dots but also add median and sem values with dots on the background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Extract selected time points for graph represented clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change figure output format from 'png' to 'pdf/svg' vectoried figure format\n",
    "%config InlineBackend.figure_formats = ['pdf','svg']\n",
    "\n",
    "## Graph theory represented cluter replot\n",
    "giant_node_replot = [] #At each time points, node index within the largest cluster\n",
    "ORDER_replot = [] #Record nodes order in the rearranged cluster graphs\n",
    "CCSLISTSIZE_replot = [] #Record cluster size at all selected timepoints\n",
    "\n",
    "selected_time = [0.01,0.25,0.50,0.77]\n",
    "Select_replot_df = df.loc[df['time'] == selected_time[0]] #Create new dataframe for replotting purpose with selected timepoints\n",
    "for i in selected_time[1:len(selected_time)]:\n",
    "    Select_replot_df = Select_replot_df.append(df.loc[df['time'] == i])\n",
    "    \n",
    "# for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "# for index, row in tqdm(df.head(n=2).iterrows(), total=2):\n",
    "for index,row in tqdm(Select_replot_df.iterrows(), total=Select_replot_df.shape[0]):\n",
    "    g = row['graph']\n",
    "    ccs = g.clusters()\n",
    "    ccslistsize = list(ccs.sizes())\n",
    "    giant_node_replot.append(ccs[ccslistsize.index(max(ccslistsize))])\n",
    "    CCSLISTSIZE_replot.append(ccslistsize)\n",
    "    \n",
    "    # Calculate shortest paths length between nodes\n",
    "    spl_total1 = []\n",
    "    N_connect_total = []\n",
    "    for v_source in range(0,g.vcount()):\n",
    "        spl = g.shortest_paths_dijkstra(v_source, g.vs(), weights=g.es['length'])\n",
    "        spl_total1.append(spl)\n",
    "        N_connect = [float(len(i)) for i in g.get_shortest_paths(v_source, g.vs())] #Number of nodes connecting v_source and v_target (including v_source and v_target)\n",
    "        N_connect_total.append(N_connect)\n",
    "    distances = np.array(spl_total1).reshape((g.vcount(),g.vcount()))\n",
    "    N_connect_total = np.array(N_connect_total).reshape((g.vcount(),g.vcount()))\n",
    "\n",
    "    # Replace infinities with a very large distance\n",
    "    distances[np.isinf(distances)] = distances[~np.isinf(distances)].max()\n",
    "    \n",
    "    # Clustering\n",
    "    threshold = 1\n",
    "    fig, axs = plt.subplots(1,2,figsize=(35,6))\n",
    "    sys.setrecursionlimit(10000) #Required for much larger system as trail20_(5-5) which has 1170 type A molecules\n",
    "    linkage = hierarchy.linkage(distances, method=\"single\")\n",
    "    clusters = hierarchy.fcluster(linkage, threshold, criterion=\"distance\")\n",
    "    dend = hierarchy.dendrogram(linkage, color_threshold=threshold, ax=axs[1])\n",
    "    order = dend['leaves']\n",
    "    ORDER_replot = np.append(ORDER_replot,order,axis=0)\n",
    "    distances = distances[order,:]\n",
    "    distances = distances[:,order]\n",
    "\n",
    "#   Plot all the analysis results  \n",
    "    #After clustering, convert the largest distances (& not indirectly connected nodes) to nan and plot as white in the colormap\n",
    "    max_index = np.where(distances == np.amax(distances)) \n",
    "    distances[max_index] = np.nan\n",
    "    no_connect_index = np.where(N_connect_total == 0)\n",
    "    N_connect_total[no_connect_index] = np.nan\n",
    "    current_cmap = plt.cm.get_cmap()\n",
    "    current_cmap.set_bad(color='white')\n",
    "    \n",
    "    [x,y] = np.where(Direct_neighbor == 1) #Extract x,y node position for those are direct neighbors\n",
    "    \n",
    "    sc0 = axs[0].imshow(distances)\n",
    "    cbar0 = fig.colorbar(sc0, ax=axs[0],extend='neither')\n",
    "    sc0.set_clim(vmin=0,vmax=0.8)\n",
    "    cbar0.set_label(r'Topological shortest distance ($\\mu m$)', fontsize=18)\n",
    "    axs[0].set_title(f't = {df.time[index]:.3f} s', fontsize=18)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Overlay the histogram of cluster size distribution at the above selected time points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for i in range(len(selected_time)):\n",
    "    plt.hist(CCSLISTSIZE_replot[i],bins = range(min(CCSLISTSIZE_replot[i]),max(CCSLISTSIZE_replot[i])+1,1),histtype='step',density=True,label=[str(selected_time[i])+'s'],fill=False)\n",
    "    \n",
    "plt.ylabel('Probability', fontsize=15)\n",
    "plt.xlabel('Cluster size', fontsize=15)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.9))\n",
    "plt.show()\n",
    "\n",
    "# print(CCSLISTSIZE_replot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Replot the cluster diffusivity with cluster size by combining all time information and use single color for each dots but also add mean(not median) and std values with dots on the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combined 1st and 2nd figure of Deff vs. cluster size without time information as colormap\n",
    "cmap = plt.cm.get_cmap('jet')\n",
    "fig, ax = plt.subplots(1,1, figsize=(6,4))\n",
    "timemax = 0.77\n",
    "timemin = max(df.time)*0\n",
    "timeindex = df_diff.time[(df_diff.time >= timemin) & (df_diff.time <= timemax)].index\n",
    "sc = ax.scatter(df_diff.nmols[timeindex],df_diff[('diffusivity','mean')][timeindex]/(1e-12), s=1, c='grey')\n",
    "\n",
    "# sc = ax.scatter(df_diff.nmols[timeindex],df_diff[('diffusivity','mean')][timeindex]/(1e-12), s=1,c=df_diff.time[timeindex], cmap=cmap)\n",
    "# cbar = plt.colorbar(sc,extend='neither')\n",
    "# sc.set_clim(vmin=timemin,vmax=timemax)\n",
    "# cbar.set_label('time (s)', fontsize=14)\n",
    "\n",
    "# Plot the average molecular diffusion constants vs the cluster size that molecules are within\n",
    "df_diff_timeindex = pd.DataFrame({\"cluster_size\": df_diff.nmols[timeindex], \"Diff_cluster_size\": df_diff[('diffusivity','mean')][timeindex]})\n",
    "Diff_cluster_size = df_diff_timeindex.groupby('cluster_size').agg(['mean','std'])\n",
    "Diff_cluster_size = Diff_cluster_size.reset_index() #Reset the dataframe to regular column\n",
    "# # Linear fitting the log scale graph (check power law index)\n",
    "# coef = np.polyfit(np.log10(Diff_cluster_size[\"cluster_size\"]),np.log10(Diff_cluster_size[('Diff_cluster_size','mean')]),1)\n",
    "# poly1d_fn = np.poly1d(coef) # poly1d_fn is now a function which takes in x and returns an estimate for y\n",
    "# Diff_cluster_size_fit = [pow(10,i)/(1e-12) for i in poly1d_fn(np.log10(Diff_cluster_size[\"cluster_size\"]))]\n",
    "# ax2.plot(Diff_cluster_size[\"cluster_size\"],Diff_cluster_size_fit,'--r')\n",
    "# ax2.text(250, 0.5, f'Exponent $\\\\alpha$={round(coef[0], 2)}')\n",
    "plt.errorbar(Diff_cluster_size[\"cluster_size\"], Diff_cluster_size[('Diff_cluster_size','mean')]/(1e-12), yerr = Diff_cluster_size[('Diff_cluster_size','std')]/(1e-12), xerr = None)\n",
    "ax.set_xlim(0.75,300)\n",
    "ax.set_ylim(0.17,1.35)\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('Cluster size (#molecules)', fontsize=18)\n",
    "ax.set_ylabel(r'Avg. diffusivity ($\\mu m^2/s$)', fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Time course of tracking the largest cluster at the last time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the number of nodes within the first tracked cluster (largest cluster at the last time point) over time\n",
    "timemax = 0.77\n",
    "timemin = max(df.time)*0\n",
    "timemax_index = int(timemax/0.01);\n",
    "timemin_index = int(timemin/0.01);\n",
    "\n",
    "dt = np.diff(reversed_df.time[-timemax_index:])\n",
    "dCs = np.diff(Cluster_size[-timemax_index:])\n",
    "fig,axs = plt.subplots(1,2, figsize=(15,6))\n",
    "axs[0].plot(reversed_df.time[-timemax_index:],Cluster_size[-timemax_index:])\n",
    "axs[0].set_ylabel(r'Number of nodes within first tracked cluster', fontsize=18)\n",
    "axs[0].set_xlabel('Time (s)', fontsize=18)\n",
    "axs[0].set_ylim(0,220)\n",
    "\n",
    "axs[1].plot(reversed_df.time[-(timemax_index-1):],[x/y for x, y in zip(dCs, dt)])\n",
    "axs[1].set_ylabel(r'First derivation of node number', fontsize=18)\n",
    "axs[1].set_xlabel('Time (s)', fontsize=18)\n",
    "plt.show()\n",
    "    \n",
    "## Change back figure output format to 'png'\n",
    "%config InlineBackend.figure_formats = ['png']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-analyze cluster diffusivity using center of mass rather than averaged individual molecular diffusivity for the paper:\n",
    "\n",
    "* only clusters with size larger than **'Minimum_size'** are considered for linking and tracking\n",
    "* All possible tracks are first identified through 'Trac_cluster_...' variables\n",
    "* 'Trac_cluster_...' are further processed through function: 'cluster_selection' to only select clusters if nodes number change between neighboring frames is smaller or equal than **'del_cluster'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pre-requisite variables:\n",
    "# df, L, ORDER, N_CONNECT_TOTAL (first calculated in cell 8,9,10)\n",
    "\n",
    "timemax = 0.77\n",
    "Select_df = df.loc[df['time'] <= timemax] #Create new dataframe for replotting purpose with selected timepoints\n",
    "\n",
    "## Initiate important variables\n",
    "Minimum_size = 10 #Minimum cluster size for mesoscale diffusivity analyses\n",
    "# All variables listed below have the same structure as list of list and values within the same position refer to the same cluster\n",
    "# list of list of list:[[[previous cluster #, current cluster #],different links],different times]\n",
    "Link_cluster_index = [] #linkage cluster index between two neighboring frames: ['PreFrame-Index_Cluster-Index-in-PreFrame','CurrentFrame-Index_Cluster-Index-in-CurrentFrame']\n",
    "Link_cluster_time = []\n",
    "Link_cluster_size = []\n",
    "Link_cluster_x = []\n",
    "Link_cluster_y = []\n",
    "Link_cluster_z = []\n",
    "\n",
    "df_copy = Select_df.copy()  #Copy dataframe 'df' to a new dataframe 'df_copy' to avoid changing original dataframe\n",
    "reversed_df = df_copy.loc[::-1] #Reverse copied dataframe in terms of index and saved to 'reversed_df' dataframe\n",
    "## Generate intial linkage variables for subsequence analyses\n",
    "# for index, row in tqdm(reversed_df.head(n=10).iterrows(), total=10):\n",
    "for index, row in tqdm(reversed_df.iterrows(), total=reversed_df.shape[0]):\n",
    "    g = row['graph']\n",
    "    Coord = g.vs['coordinate']\n",
    "    order = ORDER[index*g.vcount():(index+1)*g.vcount()] #ORDER: Record nodes order in the rearranged cluster graphs; order: has a size of g.vcount()\n",
    "    n_connect_total = N_CONNECT_TOTAL[index*g.vcount():(index+1)*g.vcount()] # N_CONNETCT_TOTAL: Record number of nodes in the shortest path connecting two nodes in all time points; n_connect_total: has a size of g.vcount() * g.vcount()\n",
    "    \n",
    "    ##Extract node index for each ordered cluster within each frame after clustering (Ni_index)\n",
    "    ccs = g.clusters()\n",
    "    N_cluster = len(ccs) #N_cluster is the number of clusters within each frame    \n",
    "    Ni_index = [] #Store the actual node index (not index directly from n_connect_total matrix since it records the number of nodes rather than actual node number) for each cluster using order array\n",
    "    check_temp_cluster = 0 #Checkpoint for the row index between different squares in graph\n",
    "    if np.any(np.isnan(n_connect_total)): #whether N_CONNECT_TOTAL elements have been replace to NaN for plotting\n",
    "        for j in range(0,N_cluster):\n",
    "            temp_cluster = [i for i, e in enumerate(n_connect_total[check_temp_cluster]) if ~np.isnan(e)]\n",
    "            Ni_index.append(order[temp_cluster])\n",
    "            check_temp_cluster = check_temp_cluster+len(temp_cluster)\n",
    "    else:\n",
    "        for j in range(0,N_cluster):\n",
    "            temp_cluster = [i for i, e in enumerate(n_connect_total[check_temp_cluster]) if e!=0]\n",
    "            Ni_index.append(order[temp_cluster])\n",
    "            check_temp_cluster = check_temp_cluster+len(temp_cluster)\n",
    "\n",
    "    Len_Ni_index = np.vectorize(len)(Ni_index) #Size of each sorted cluster using np.array\n",
    "    \n",
    "    ##Identify clusters that are larger than Minimum_size for each frame and setup initial pre variables at the last frame\n",
    "    if index == reversed_df.shape[0]-1:\n",
    "        temp_index_pre = np.where(Len_Ni_index > Minimum_size) #Only track those larger clusters\n",
    "        temp_cluster_pre = [] #Record nodes number within each cluster\n",
    "        t_pre = reversed_df.time[index]\n",
    "        for i in temp_index_pre[0]:\n",
    "            temp_cluster_pre.append(Ni_index[i])\n",
    "    else:            \n",
    "        temp_index = np.where(Len_Ni_index > Minimum_size) \n",
    "        temp_cluster = []\n",
    "        t_now = reversed_df.time[index]\n",
    "        for i in temp_index[0]:\n",
    "            temp_cluster.append(Ni_index[i])\n",
    "    \n",
    "    ##Create the linkage variables as list of list of list:[[[previous cluster #, current cluster #],different links],different times] (Link_cluster_index & Link_cluster_time)\n",
    "    if index != reversed_df.shape[0]-1: #Compare only when temp_cluster has been created\n",
    "        Link_cluster_index_temp = [] #Create temperal cluster linkage between each neighboring frames\n",
    "        Link_cluster_time_temp = []\n",
    "        Link_cluster_size_temp = []\n",
    "        Link_cluster_x_temp = []\n",
    "        Link_cluster_y_temp = []\n",
    "        Link_cluster_z_temp = []\n",
    "        for j in range(len(temp_cluster_pre)):\n",
    "            j_node = temp_cluster_pre[j]\n",
    "            cluster_coord_pre = [Coord[int(i)] for i in j_node]\n",
    "            com_pre = np.average(cluster_coord_pre, axis=0)\n",
    "            common_nodes = []\n",
    "            for i in range(len(temp_cluster)):\n",
    "                i_node = temp_cluster[i]\n",
    "                common_nodes.append(len(np.intersect1d(i_node, j_node)))\n",
    "            if len(common_nodes) !=0: #Save when there are clusters with size larger than Minimum_size\n",
    "                if max(common_nodes) != 0: #When common_nodes is not empty and there is an overlap\n",
    "                    max_node_index = common_nodes.index(max(common_nodes)) #This index is used for current cluster\n",
    "                    Link_index1 = '_'.join([f'{index+1}', f'{j}']) #Index is in the reverse order\n",
    "                    Link_index2 = '_'.join([f'{index}', f'{max_node_index}'])\n",
    "                    Link_cluster_index_temp.append([Link_index1,Link_index2])\n",
    "                    Link_cluster_time_temp.append([t_pre,t_now])\n",
    "                    Link_cluster_size_temp.append([len(j_node),len(temp_cluster[max_node_index])])\n",
    "                    \n",
    "                    cluster_coord = [Coord[int(i)] for i in temp_cluster[max_node_index]]\n",
    "                    com = np.average(cluster_coord, axis=0)\n",
    "                    Link_cluster_x_temp.append([com_pre[0],com[0]])\n",
    "                    Link_cluster_y_temp.append([com_pre[1],com[1]])\n",
    "                    Link_cluster_z_temp.append([com_pre[2],com[2]])\n",
    "                                               \n",
    "        \n",
    "        #Append the temperal cluster linkage to the global variables\n",
    "        Link_cluster_index.append(Link_cluster_index_temp)\n",
    "        Link_cluster_time.append(Link_cluster_time_temp)\n",
    "        Link_cluster_size.append(Link_cluster_size_temp)\n",
    "        Link_cluster_x.append(Link_cluster_x_temp)\n",
    "        Link_cluster_y.append(Link_cluster_y_temp)\n",
    "        Link_cluster_z.append(Link_cluster_z_temp)\n",
    "       \n",
    "        #Overwrite the temp_cluster_pre & t_pre and prepare for comparison between next two frames\n",
    "        temp_cluster_pre = temp_cluster\n",
    "        t_pre = t_now\n",
    "        \n",
    "# print(Link_cluster_index)\n",
    "# print(Link_cluster_time)\n",
    "# print(Link_cluster_size)\n",
    "# print('-----------------------------------------')\n",
    "\n",
    "## Retransform the linkage variables to the actual cluster trackings\n",
    "# All variables listed below have the same structure as list of list and values within the same position refer to the same cluster\n",
    "Trac_cluster_index = [] #index identified in the Link_cluster_index rearrange into identified tracks\n",
    "Trac_cluster_size = [] #Number of nodes within each tracked cluster, [[one cluster at different time],different clusters]\n",
    "Trac_cluster_t = [] #Time information when cluster is recorded\n",
    "Trac_cluster_x = [] #Center of mass for each tracked cluster\n",
    "Trac_cluster_y = []\n",
    "Trac_cluster_z = []\n",
    "\n",
    "for k in range(len(Link_cluster_index[0])):\n",
    "    Trac_cluster_index.append(Link_cluster_index[0][k].copy()) #'.copy' used here is because 'append' is adding list address not values. If not used, 'append' will also change list itself\n",
    "    Trac_cluster_size.append(Link_cluster_size[0][k].copy())\n",
    "    Trac_cluster_t.append(Link_cluster_time[0][k].copy())\n",
    "    Trac_cluster_x.append(Link_cluster_x[0][k].copy())\n",
    "    Trac_cluster_y.append(Link_cluster_y[0][k].copy())\n",
    "    Trac_cluster_z.append(Link_cluster_z[0][k].copy())\n",
    "\n",
    "for i in range(1, len(Link_cluster_index)):\n",
    "    for j in range(len(Link_cluster_index[i])):\n",
    "        pathFound = False\n",
    "        for k in range(len(Trac_cluster_index)):\n",
    "            if Trac_cluster_index[k][-1] == Link_cluster_index[i][j][0]:\n",
    "                Trac_cluster_index[k].append(Link_cluster_index[i][j][1])\n",
    "                Trac_cluster_size[k].append(Link_cluster_size[i][j][1])\n",
    "                Trac_cluster_t[k].append(Link_cluster_time[i][j][1])\n",
    "                Trac_cluster_x[k].append(Link_cluster_x[i][j][1])\n",
    "                Trac_cluster_y[k].append(Link_cluster_y[i][j][1])\n",
    "                Trac_cluster_z[k].append(Link_cluster_z[i][j][1])\n",
    "                pathFound = True\n",
    "        if pathFound == False:\n",
    "            Trac_cluster_index.append(Link_cluster_index[i][j].copy())\n",
    "            Trac_cluster_size.append(Link_cluster_size[i][j].copy())\n",
    "            Trac_cluster_t.append(Link_cluster_time[i][j].copy())\n",
    "            Trac_cluster_x.append(Link_cluster_x[i][j].copy())\n",
    "            Trac_cluster_y.append(Link_cluster_y[i][j].copy())\n",
    "            Trac_cluster_z.append(Link_cluster_z[i][j].copy())\n",
    "\n",
    "# print('-----------------------------------------')\n",
    "# print(Trac_cluster_index)\n",
    "# print(Trac_cluster_size)\n",
    "# print(Trac_cluster_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "## Calculate MSD for a particular known trajectory of cluster center of mass\n",
    "def MSD_boundary_condition(trajectory_x, trajectory_y, trajectory_z, trajectory_t):\n",
    "    MSD = np.zeros(len(trajectory_t)-1)\n",
    "    tau = np.zeros(len(trajectory_t)-1)\n",
    "    for i in range(len(trajectory_t)-1):\n",
    "        tau_temp = np.mean([i-j for i,j in zip(trajectory_t[:-(i+1)],trajectory_t[(i+1):])])\n",
    "        tau[i] = tau_temp\n",
    "        dx = np.array([i-j for i,j in zip(trajectory_x[:-(i+1)],trajectory_x[(i+1):])])\n",
    "        dy = np.array([i-j for i,j in zip(trajectory_y[:-(i+1)],trajectory_y[(i+1):])])\n",
    "        dz = np.array([i-j for i,j in zip(trajectory_z[:-(i+1)],trajectory_z[(i+1):])])\n",
    "        MSD[i] = np.mean(np.sqrt(dx*dx + dy*dy + dz*dz))\n",
    "    return [MSD,tau]\n",
    "\n",
    "## Self defined criteria of selecting partial cluster trajectory for MSD calculations\n",
    "def cluster_selection(cluster_size, cluster_x, cluster_y, cluster_z, cluster_t, del_cluster):\n",
    "    size_change = np.abs(np.diff(cluster_size))\n",
    "    idx = np.where(size_change > del_cluster)[0]\n",
    "    if len(idx) != 0: #If index is not an empty array\n",
    "        if idx[0] !=0: #If the first element is not 0\n",
    "            idx = np.insert(idx,0,-1)\n",
    "        cluster_size_update = []\n",
    "        cluster_x_update = []\n",
    "        cluster_y_update = []\n",
    "        cluster_z_update = []\n",
    "        cluster_t_update = []\n",
    "        for i in range(len(idx)-1):\n",
    "            cluster_size_update.append(cluster_size[idx[i]+1:idx[i+1]+1])\n",
    "            cluster_x_update.append(cluster_x[idx[i]+1:idx[i+1]+1])\n",
    "            cluster_y_update.append(cluster_y[idx[i]+1:idx[i+1]+1])\n",
    "            cluster_z_update.append(cluster_z[idx[i]+1:idx[i+1]+1])\n",
    "            cluster_t_update.append(cluster_t[idx[i]+1:idx[i+1]+1])\n",
    "    else:\n",
    "        cluster_size_update = [cluster_size] #Create list of list for easier processing afterwards\n",
    "        cluster_x_update = [cluster_x]\n",
    "        cluster_y_update = [cluster_y]\n",
    "        cluster_z_update = [cluster_z]\n",
    "        cluster_t_update = [cluster_t]\n",
    "    return cluster_size_update, cluster_x_update, cluster_y_update, cluster_z_update, cluster_t_update\n",
    "\n",
    "# Calculate Deff for each cluster and plot the trajectories for all tracked cluster\n",
    "L_trac_cutoff = 10\n",
    "del_cluster = 20; #Cluster size change between neighboring time point should be smaller than 'del_cluster'\n",
    "TOTAL_MSD = [] # Save all MSD for individual tracks\n",
    "TOTAL_TAU = []\n",
    "Deff = []\n",
    "Deff_cluster_size = []\n",
    "for i in range(len(Trac_cluster_size)):\n",
    "    [cluster_size_update, cluster_x_update, cluster_y_update, cluster_z_update, cluster_t_update] = cluster_selection(Trac_cluster_size[i], Trac_cluster_x[i], Trac_cluster_y[i], Trac_cluster_z[i], Trac_cluster_t[i],del_cluster)\n",
    "    for j in range(len(cluster_size_update)):\n",
    "        if len(cluster_size_update[j]) >= L_trac_cutoff:\n",
    "            x = cluster_x_update[j]\n",
    "            y = cluster_y_update[j]\n",
    "            z = cluster_z_update[j]\n",
    "            clus_size = cluster_size_update[j]\n",
    "            t = cluster_t_update[j]\n",
    "            [MSD,tau] = MSD_boundary_condition(x,y,z,t)\n",
    "            TOTAL_MSD.append(MSD)\n",
    "            TOTAL_TAU.append(tau)\n",
    "            \n",
    "            #Fitting to get Effective Deff\n",
    "            coef = np.polyfit(tau[:L_trac_cutoff],MSD[:L_trac_cutoff],1) #Use MSD=6*D*tau+error for fitting\n",
    "            poly1d_fn = np.poly1d(coef)\n",
    "            MSD_fit = [i for i in poly1d_fn(tau)]\n",
    "            Deff.append(coef[0]/6)\n",
    "            Deff_cluster_size.append(np.mean(clus_size))\n",
    "\n",
    "            fig = plt.figure(figsize=(15,10))\n",
    "            ax1 = fig.add_subplot(221, projection='3d')\n",
    "            scatter1 = ax1.scatter(x, y, z, c=clus_size, marker='o', cmap='viridis')\n",
    "            cbar1 = plt.colorbar(scatter1)\n",
    "            cbar1.set_label('Cluster Size') \n",
    "\n",
    "            ax2 = fig.add_subplot(222)\n",
    "            colormap = ListedColormap(plt.cm.viridis(1 - (t-min(t))/(max(t)-min(t))))\n",
    "            scatter2 = ax2.scatter(x, y, c=t, cmap=colormap, marker='.', s=100)\n",
    "            cbar2 = plt.colorbar(scatter2)\n",
    "            cbar2.set_label('Time')\n",
    "\n",
    "            ax3 = fig.add_subplot(223)\n",
    "            plt.plot(tau, MSD, marker='o')\n",
    "            plt.plot(tau,MSD_fit,'--r')\n",
    "\n",
    "\n",
    "            ax4 = fig.add_subplot(224)\n",
    "            plt.plot(t, clus_size, marker='o')\n",
    "\n",
    "            # Add labels and title\n",
    "            ax1.set_xlabel('Box X ($\\mu$m)')\n",
    "            ax1.set_ylabel('Box Y ($\\mu$m)')\n",
    "            ax1.set_zlabel('Box Z ($\\mu$m)')\n",
    "\n",
    "            ax2.set_xlabel('Box X ($\\mu$m)')\n",
    "            ax2.set_ylabel('Box Y ($\\mu$m)')\n",
    "\n",
    "            ax3.set_xlabel(r'$\\tau$ (s)')\n",
    "            ax3.set_ylabel(r'MSD ($\\mu$m$^2$)')\n",
    "            \n",
    "            ax4.set_xlabel('time (s)')\n",
    "            ax4.set_ylabel('Cluster Size')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "scatter = ax.scatter(Deff_cluster_size, Deff)\n",
    "plt.xlabel('Cluster size')\n",
    "plt.ylabel(r'$D_{eff} $ $\\mu$m$^2/s$')\n",
    "plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "# ## Below is to determine what's proper values for cluster size change between neighboring frames by changing bins value\n",
    "# dClust = []\n",
    "# for size in Trac_cluster_size:\n",
    "#     dClust.append(np.diff(size))\n",
    "    \n",
    "# dClust_combined = np.abs(np.concatenate(dClust))\n",
    "# bin_width = 10\n",
    "# num_bins = int((dClust_combined.max() - dClust_combined.min()) / bin_width)\n",
    "# fig = plt.figure()\n",
    "# hist, bin_edges, _ = plt.hist(dClust_combined, bins=num_bins) #bins = 10-20 could cover most size changes between neighboring frames\n",
    "# print(\"Histogram Values (Frequencies):\", hist)\n",
    "# plt.xlabel('Cluster size change between neighboring frame')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the following outputs and parameters into an excel file: gsd_file, Minimum_size, del_cluster, L_trac_cutoff, Deff_cluster_size, Deff\n",
    "\n",
    "output = {'Deff': Deff, 'Deff_cluster_size': Deff_cluster_size}\n",
    "df_output = pd.DataFrame(output)\n",
    "df_output['filename'] = Filename\n",
    "df_output['Minimum_cluster_size'] = Minimum_size\n",
    "df_output['Maximum_cluster_rate_change'] = del_cluster\n",
    "df_output['Minimum_track_length'] = L_trac_cutoff\n",
    "\n",
    "Output_folder_path = f\"/Users/shut01/Documents/Levy simulation folder/Re-analyze simulation data/{Filename}\"\n",
    "csv_filename = f\"{Output_folder_path}/Output_Mesoscale_Diffusivity_{Filename}.csv\"\n",
    "df_output.to_csv(csv_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
